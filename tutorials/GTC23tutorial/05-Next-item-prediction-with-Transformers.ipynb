{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e96de18a-caae-4029-bb74-18b6751c123d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ================================\n",
    "\n",
    "# Each user is responsible for checking the content of datasets and the\n",
    "# applicable licenses and determining if suitable for the intended use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964146b4-8561-4810-8e8e-368cd4828afb",
   "metadata": {},
   "source": [
    "## Next item prediction with a Transformer-based model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e7ce3a-c387-45e3-8a04-62abc1c50cc4",
   "metadata": {},
   "source": [
    "In recent years, several deep learning-based algorithms have been proposed for recommendation systems while its adoption in industry deployments have been steeply growing. In particular, NLP inspired approaches have been successfully adapted for sequential and session-based recommendation problems, which are important for many domains like e-commerce, news and streaming media. Session-Based Recommender Systems (SBRS) have been proposed to model the sequence of interactions within the current user session, where a session is a short sequence of user interactions typically bounded by user inactivity. They have recently gained popularity due to their ability to capture short-term or contextual user preferences towards items.\n",
    "\n",
    "The field of NLP has evolved significantly within the last decade, particularly due to the increased usage of deep learning. As a result, state of the art NLP approaches have inspired RecSys practitioners and researchers to adapt those architectures, especially for sequential and session-based recommendation problems. Here, we use one of the state-of-the-art Transformer-based architecture, XLNet with Causal Language Modeling (CLM) training technique for multi-class classification task. For this, we leverage the popular HuggingFace’s Transformers NLP library and make it possible to experiment with cutting-edge implementation of such architectures for sequential and session-based recommendation problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ece498b-8775-4448-b840-a0e41bb98254",
   "metadata": {},
   "source": [
    "### 5.1.1. What's Transformers?\n",
    "The Transformer is a competitive alternative to the models using Recurrent Neural Networks (RNNs) for a range of sequence modeling tasks. The Transformer architecture [6] was introduced as a novel architecture in NLP domain that aims to solve sequence-to-sequence tasks relying entirely on self-attention mechanism to compute representations of its input and output. Hence, the Transformer overperforms RNNs with their three mechanisms:\n",
    "\n",
    "- Non-sequential: Transformers network is parallelized where as RNN computations are inherently sequential. That resulted in significant speed-up in the training time.<br>\n",
    "- Self-attention mechanisms: Transformers rely entirely on self-attention mechanisms that directly model relationships between all item-ids in a sequence.\n",
    "- Positional encodings: A representation of the location or “position” of items in a sequence which is used to give the order context to the model architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b97311-4345-4b16-866d-138ebe670767",
   "metadata": {},
   "source": [
    "**Learning Objectives:**\n",
    "- Train and evaluate a transformer-based model (XLNet) for next-item prediction task\n",
    "- Apply weight-tying technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02f47581-aa15-4796-9648-f5f00713de6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"]=\"cuda_malloc_async\"\n",
    "import gc\n",
    "import numpy as np\n",
    "\n",
    "from merlin.schema.tags import Tags\n",
    "from merlin.io.dataset import Dataset\n",
    "\n",
    "import merlin.models.tf as mm\n",
    "from merlin.models.tf.core.aggregation import SequenceAggregator\n",
    "from merlin.models.tf.transforms.tensor import ListToDense, ListToRagged\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfa6e4fb-8905-403c-83aa-80daddbc45a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "286e75c1-dd7c-49af-b3bf-ecd9d402079f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = os.environ.get(\n",
    "    \"DATA_FOLDER\", \n",
    "    '/workspace/data'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb714228-92d5-4e6a-a8fc-b618727c1b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:148: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train = Dataset(os.path.join(DATA_FOLDER, \"train/*.parquet\"))\n",
    "valid = Dataset(os.path.join(DATA_FOLDER, \"valid/*.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0414de6b-e5b8-4d69-88e0-afd70b63d7e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'city_id_list'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = train.schema.select_by_tag(Tags.SEQUENCE).column_names[0]\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2d9d322-6348-4c78-af91-3aafd9a0211b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = int(os.environ.get(\n",
    "    \"EPOCHS\", \n",
    "    '3'\n",
    "))\n",
    "\n",
    "dmodel = int(os.environ.get(\n",
    "    \"dmodel\", \n",
    "    '64'\n",
    "))\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "LEARNING_RATE = 0.003\n",
    "DROPOUT = 0.0\n",
    "LABEL_SMOOTHING = 0.2\n",
    "TEMPERATURE_SCALING = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e7c250f-89ef-488f-bce3-74bd0b671a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_dims = {\n",
    "    'city_id_list': dmodel, \n",
    "    'hotel_country_list' :16\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4419ab05-737e-4fe9-aecc-3337650a3a3d",
   "metadata": {},
   "source": [
    "### XLNET MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a668316-ca22-458c-a9d1-f2858f31a6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = Dataset(os.path.join(DATA_FOLDER, \"train/*.parquet\"))\n",
    "valid = Dataset(os.path.join(DATA_FOLDER, \"valid/*.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e423fd0a-f669-482d-9941-833f2f63471a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.schema = train.schema.select_by_name(['city_id_list','booker_country_list', 'hotel_country_list',\n",
    "                                            'weekday_checkin_list','weekday_checkout_list',\n",
    "                                            'month_checkin_list','num_city_visited', 'length_of_stay_list']\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6230f288-5178-4148-af1c-8b743c9e0a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_schema =train.schema.select_by_tag(Tags.SEQUENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "870bbe49-a31a-41d3-8828-e3c8700ff77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_schema = train.schema.select_by_tag(Tags.CONTEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a89967ee-6026-45f4-a971-f26eec08c696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'city_id_list'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_schema = train.schema.select_by_tag(Tags.ITEM_ID)\n",
    "target = target_schema.column_names[0]\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9cce3f7-b001-4609-a7a0-99df20bdbff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_block = mm.MLPBlock(\n",
    "                [128,dmodel],\n",
    "                activation='relu',\n",
    "                no_activation_last_layer=True,\n",
    "                dropout=DROPOUT,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f54394-4c03-4f98-91cf-c0678f2a2cbe",
   "metadata": {},
   "source": [
    "Let's create a sequential block where we connect sequential inputs block (i.e., a SequentialLayer represents a sequence of Keras layers) with MLPBlock and then XLNetBlock. MLPBlock is used as a projection block to match the output dimensions of the seq_inputs block with the transformer block. In otherwords, due to residual connection in the Transformer model, we add an MLPBlock in the model pipeline. The output dim of the input block should match with the hidden dimension (d_model) of the XLNetBlock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91901797-b793-4e83-bb3a-f31b6ff6af26",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_block = mm.InputBlockV2(\n",
    "    train.schema,\n",
    "    embeddings=mm.Embeddings(\n",
    "        seq_schema.select_by_tag(Tags.CATEGORICAL), \n",
    "        sequence_combiner=None,\n",
    "        dim=manual_dims\n",
    "        ),\n",
    "    post=mm.BroadcastToSequence(context_schema, seq_schema),\n",
    ")\n",
    "\n",
    "dense_block =mm.SequentialBlock(\n",
    "    input_block,\n",
    "    mlp_block,\n",
    "    mm.XLNetBlock(d_model=dmodel, n_head=4, n_layer=2, \n",
    "                   pre=mm.ReplaceMaskedEmbeddings(),\n",
    "                   post=\"inference_hidden_state\",\n",
    "                  )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72819920-acf5-456a-8d1e-62af909d7975",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch= mm.sample_batch(train, batch_size=128, include_targets=False, to_ragged=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3a8c1339-573a-4670-913b-7de35d246283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([128, None, 114])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_block(batch).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4978f2ed-3db4-49c5-a86e-950246201c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_block2 = mm.MLPBlock(\n",
    "                [128,dmodel],\n",
    "                activation='relu',\n",
    "                no_activation_last_layer=True,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61eef09b-5eb1-4031-8119-16ec723d3437",
   "metadata": {},
   "source": [
    "CategoricalOutput class has the functionality to do weight-tying, when we provide the EmbeddingTable related to the target feature in the `to_call` method. \n",
    "\n",
    "**Weight Tying:** Sharing the weight matrix between input-to-embedding layer and output-to-softmax layer; That is, instead of using two weight matrices, we just use only one weight matrix. The intuition behind doing so is to combat the problem of overfitting. Thus, weight tying can be considered as a form of regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1abb3a2f-9086-447e-8c05-70f240d1e2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "city_id\n"
     ]
    }
   ],
   "source": [
    "item_id_name = train.schema.select_by_tag(Tags.ITEM_ID).first.properties['domain']['name']\n",
    "print(item_id_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18c63055-b97e-4d47-b443-95bf0cedb57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_task= mm.CategoricalOutput(\n",
    "    to_call=input_block[\"categorical\"][item_id_name],\n",
    "    logits_temperature=TEMPERATURE_SCALING,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e551f18-580a-4f37-943f-cdbf3dfb5315",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_transformer = mm.Model(dense_block, mlp_block2, prediction_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d8d3a0a-f270-4f0f-8766-4bb86e563d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=LEARNING_RATE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a812c452-1073-4819-8bc2-8eead48d7ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:148: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/keras/initializers/initializers_v2.py:120: UserWarning: The initializer TruncatedNormal is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n",
      "2023-02-23 00:26:28.697454: I tensorflow/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model/sequential_block_4/xl_net_block/prepare_transformer_inputs_4/RaggedToTensor/boolean_mask_1/GatherV2:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model/sequential_block_4/xl_net_block/prepare_transformer_inputs_4/RaggedToTensor/boolean_mask/GatherV2:0\", shape=(None, 64), dtype=float32), dense_shape=Tensor(\"gradient_tape/model/sequential_block_4/xl_net_block/prepare_transformer_inputs_4/RaggedToTensor/Shape:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model/sequential_block_4/xl_net_block/replace_masked_embeddings/RaggedWhere/Reshape_3:0\", shape=(None,), dtype=int64), values=Tensor(\"gradient_tape/model/sequential_block_4/xl_net_block/replace_masked_embeddings/RaggedWhere/Reshape_2:0\", shape=(None, None), dtype=float32), dense_shape=Tensor(\"gradient_tape/model/sequential_block_4/xl_net_block/replace_masked_embeddings/RaggedWhere/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model/sequential_block_4/xl_net_block/replace_masked_embeddings/RaggedWhere/RaggedTile_2/Reshape_3:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model/sequential_block_4/xl_net_block/replace_masked_embeddings/RaggedWhere/RaggedTile_2/Reshape_2:0\", shape=(None, 64), dtype=float32), dense_shape=Tensor(\"gradient_tape/model/sequential_block_4/xl_net_block/replace_masked_embeddings/RaggedWhere/RaggedTile_2/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['model/mask_emb:0', 'transformer/layer_._0/rel_attn/r_s_bias:0', 'transformer/layer_._0/rel_attn/seg_embed:0', 'transformer/layer_._1/rel_attn/r_s_bias:0', 'transformer/layer_._1/rel_attn/seg_embed:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['model/mask_emb:0', 'transformer/layer_._0/rel_attn/r_s_bias:0', 'transformer/layer_._0/rel_attn/seg_embed:0', 'transformer/layer_._1/rel_attn/r_s_bias:0', 'transformer/layer_._1/rel_attn/seg_embed:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-23 00:26:53.317384: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: model/sequential_block_4/xl_net_block/replace_masked_embeddings/RaggedWhere/Assert/AssertGuard/branch_executed/_170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "409/409 [==============================] - 94s 158ms/step - loss: 0.8112 - recall_at_4: 0.0613 - mrr_at_4: 0.0355 - ndcg_at_4: 0.0420 - map_at_4: 0.0355 - precision_at_4: 0.0153 - regularization_loss: 0.0000e+00 - loss_batch: 0.8107\n",
      "Epoch 2/3\n",
      "409/409 [==============================] - 68s 159ms/step - loss: 0.6346 - recall_at_4: 0.2447 - mrr_at_4: 0.1547 - ndcg_at_4: 0.1774 - map_at_4: 0.1547 - precision_at_4: 0.0612 - regularization_loss: 0.0000e+00 - loss_batch: 0.6344\n",
      "Epoch 3/3\n",
      "409/409 [==============================] - 68s 159ms/step - loss: 0.5698 - recall_at_4: 0.3174 - mrr_at_4: 0.2036 - ndcg_at_4: 0.2322 - map_at_4: 0.2036 - precision_at_4: 0.0793 - regularization_loss: 0.0000e+00 - loss_batch: 0.5696\n",
      "CPU times: user 4min 49s, sys: 36.1 s, total: 5min 26s\n",
      "Wall time: 4min 7s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8802818e50>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model_transformer.compile(run_eagerly=False, optimizer=optimizer, loss=\"categorical_crossentropy\",\n",
    "              metrics=mm.TopKMetricsAggregator.default_metrics(top_ks=[4])\n",
    "             )\n",
    "model_transformer.fit(train, batch_size=512, epochs=3, pre=mm.SequenceMaskRandom(schema=seq_schema, target=target, masking_prob=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "643fdac2-52f5-4b8f-b818-85564d2ef271",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_last = mm.SequenceMaskLast(schema=seq_schema, target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99699e36-2f4e-4bf3-84ef-049d652298fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid.schema = train.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c3b6cbd-dd26-4317-8160-2222b77d08fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-23 00:30:27.757963: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: model/sequential_block_4/xl_net_block/replace_masked_embeddings/RaggedWhere/Assert/AssertGuard/branch_executed/_131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 11s 163ms/step - loss: 0.3794 - recall_at_4: 0.4719 - mrr_at_4: 0.3207 - ndcg_at_4: 0.3588 - map_at_4: 0.3207 - precision_at_4: 0.1180 - regularization_loss: 0.0000e+00 - loss_batch: 0.3765\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.3793621063232422,\n",
       " 'recall_at_4': 0.47486501932144165,\n",
       " 'mrr_at_4': 0.32167568802833557,\n",
       " 'ndcg_at_4': 0.3602368235588074,\n",
       " 'map_at_4': 0.32167568802833557,\n",
       " 'precision_at_4': 0.11871625483036041,\n",
       " 'regularization_loss': 0.0,\n",
       " 'loss_batch': 0.3642217814922333}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_transformer.evaluate(\n",
    "    valid,\n",
    "    batch_size=1024,\n",
    "    pre=predict_last,\n",
    "    return_dict=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7a732e-8d72-4d5e-8710-78f2bacd394f",
   "metadata": {},
   "source": [
    "- ReplaceMaskedEmbeddings and SequenceMaskRandom are used together: replacement embeddings are learned during the training\n",
    "- ReplaceMaskedEmbeddings and SequencePredictNext are used together: replacement embeddings are not learned during the training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6952a0b9-5d5a-4380-9e0e-e5930354d271",
   "metadata": {},
   "source": [
    "we can have an xlnet with `SequencePredictLast` and with a post with sequence summary set to last. This is about how we learn the representation of the sequence. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
