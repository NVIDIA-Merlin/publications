{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f62c3c8c-b14e-4933-8b70-fc5a833f2e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ====="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4cca29-4c9e-411d-8aa8-dd10b7d7aa00",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Training and Deploying Multi-Stage Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc0dc90-36d5-4789-88b0-329bda426427",
   "metadata": {
    "tags": []
   },
   "source": [
    "Industrial recommender systems are made up of complex pipelines requiring multiple steps including feature engineering and\n",
    "preprocessing, a retrieval model for candidate generation, filtering, a feature store query, a ranking model for scoring, and an ordering\n",
    "stage. These pipelines need to be carefully deployed as a set, requiring coordination during their development and deployment. Data\n",
    "scientists, ML engineers, and researchers might focus on different stages of recommender systems, however they share a common\n",
    "desire to reduce the time and effort searching for and combining boilerplate code coming from different sources or writing custom\n",
    "code from scratch to create their own RecSys pipelines.\n",
    "\n",
    "This tutorial introduces the Merlin framework which aims to make the development and deployment of recommender systems\n",
    "easier, providing methods for evaluating existing approaches, developing new ideas and deploying them to production. There are\n",
    "many techniques, such as different model architectures (e.g. MF, DLRM, DCN, etc), negative sampling strategies, loss functions or\n",
    "prediction tasks (binary, multi-class, multi-task) that are commonly used in these pipelines. Merlin provides building blocks that allow\n",
    "RecSys practitioners to focus on the “what” question in designing their model pipeline instead of “how”. Supporting research into new\n",
    "ideas within the RecSys spaces is equally important and Merlin supports the addition of custom components and the extension of\n",
    "existing ones to address gaps.\n",
    "\n",
    "In this tutorial, participants will learn: \n",
    "   - how to easily implement common recommender system techniques for comparison, \n",
    "   - how to modify components to evaluate new ideas,\n",
    "   - deploying recommender systems, bringing new ideas to production- using an open source framework Merlin and its libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbbc0ed-0253-4f56-a368-a5536c77e8e9",
   "metadata": {},
   "source": [
    "## 2. Implementing popular RecSys architectures and algorithms with Merlin Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbe7611-8697-487d-983c-a8ec0187f773",
   "metadata": {},
   "source": [
    "**Learning Objectives**\n",
    "\n",
    "- Introduction to the open source framework Merlin and its libraries- NVTabular and Merlin Models\n",
    "- Pre-processing and feature engineering with NVTabular\n",
    "- Build and train common recommender models with Merlin Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d509fb0f-c373-40ae-9dc7-799db33ed2d5",
   "metadata": {},
   "source": [
    "### NVIDIA Merlin\n",
    "\n",
    "Merlin is an open-source framework for building large-scale (deep learning) recommender systems. It is designed to support recommender systems end-to-end from ETL to training to deployment on CPU or GPU. Common deep learning frameworks are integrated such as TensorFlow or PyTorch. Its key benefits are the easy-to-use APIs, accelerations with GPU and scaling to multi-GPU or multi-node systems.\n",
    "\n",
    "![Merlin](./images/Merlin.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ba0a6a-f645-4d60-a404-fea022bac596",
   "metadata": {},
   "source": [
    "### Merlin Models\n",
    "\n",
    "[Merlin Models](https://github.com/NVIDIA-Merlin/models) is a library to make it easy for users in industry or academia to train and deploy recommender models with best practices baked into the library. This will let users in industry easily train standard models against their own dataset, getting high performance GPU accelerated models into production. This will also let researchers to build custom models by incorporating standard components of deep learning recommender models, and then benchmark their new models on example offline datasets. Core features are:\n",
    "- Unified API enables users to create models in TensorFlow or PyTorch\n",
    "- Deep integration with NVTabular for ETL and model serving\n",
    "- Flexible APIs targeted to both production and research\n",
    "- Many different recommender system architectures (tabular, two-tower, sequential) or tasks (binary, multi-class classification, multi-task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739671d0-8799-4803-9b09-aac6028a0786",
   "metadata": {},
   "source": [
    "### NVTabular "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e483dd-0388-4fd8-9047-6782ab0a2af4",
   "metadata": {},
   "source": [
    "[NVTabular](https://github.com/NVIDIA-Merlin/NVTabular) is a feature engineering and preprocessing library for tabular data that is designed to easily manipulate terabyte scale datasets and train deep learning (DL) based recommender systems. It provides high-level abstraction to simplify code and accelerates computation on the GPU using the RAPIDS Dask-cuDF library. NVTabular helps data scientists and ML engineers to:\n",
    "- process datasets that exceed GPU and CPU memory without having to worry about scale\n",
    "- focus on what to do with the data and not how to do it by using abstraction at the operation level\n",
    "- prepare datasets quickly and easily for experimentation so that more models can be trained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f32539f-a052-4d69-b6d5-17feeb9107a3",
   "metadata": {},
   "source": [
    "![Merlin](./images/schema.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c147d969-34c6-4cd4-87b3-d6f12a8d5aeb",
   "metadata": {},
   "source": [
    "That's a short introduction into Merlin, NVTabular and Merlin Models. If you are interested to learn more, we provide many examples in our GitHub repositories. \n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9d46aa-e234-4a7e-bed5-46bc0d7a4c21",
   "metadata": {},
   "source": [
    "### 2.1. Feature Engineering on GPU with NVTabular"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679905dc-c449-4827-a7bc-52164ed05480",
   "metadata": {},
   "source": [
    "In this hands-on tutorial, we use a publicly available [eCommerce behavior dataset](https://www.kaggle.com/mkechinov/ecommerce-behavior-data-from-multi-category-store). The full dataset contains 7 months data (from October 2019 to April 2020) from a large multi-category online store. Each row in the file represents an event. All events are related to products and users. Each event is like many-to-many relation between products and users. Data collected by Open CDP project and the source of the dataset is REES46 Marketing Platform.\n",
    "\n",
    "We use csv files from 2019-Oct to 2020-April for training and validating our models, so you can visit this site and download the csv files from [Kaggle](https://www.kaggle.com/mkechinov/ecommerce-behavior-data-from-multi-category-store). \n",
    "\n",
    "We already performed certain preprocessing steps on the csv files of the raw dataset. You can visit this notebook `01-Data-preparation.ipynb` to go through the code to create the `train` and `valid` parquet files that we are using in this notebook and in the following ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be2fd7e-f454-41fa-b268-8e308ee5e473",
   "metadata": {},
   "source": [
    "Below, we do following data operations with NVTabular:\n",
    "\n",
    "- Categorify Categories\n",
    "- Create temporal features\n",
    "- Apply a user defined function with LambdaOp and transform Continuous features\n",
    "- Target Encoding\n",
    "- Tagging input feature and target columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc11ccd-65e9-4d20-a30c-a850ee463c04",
   "metadata": {},
   "source": [
    "In this lab we show how to use NVTabular operations for prepocessing and feature engineering, but we are not going into details of NVTabular. To learn more about NVTabular operators please visit the [documentation](https://nvidia-merlin.github.io/NVTabular/main/Introduction.html) page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b54d3c6-fe6c-4a41-ab5e-c80c92efd91d",
   "metadata": {},
   "source": [
    "**Import Required Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1423d7a5-1ec7-4c9b-a65d-4ac6216f9817",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-09 15:42:28.712492: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-09 15:42:31.792996: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 16255 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB-LS, pci bus id: 0000:8a:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import glob\n",
    "import cudf \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nvtabular as nvt\n",
    "from nvtabular.ops import *\n",
    "import gc\n",
    "\n",
    "\n",
    "from merlin.schema.tags import Tags\n",
    "import merlin.models.tf as mm\n",
    "from merlin.io.dataset import Dataset\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a85c3587-4c2f-41b8-bf64-a15a6fe73c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8efaea78-8cc2-4483-a200-21fbb88f1125",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/workspace/data/ecom/'\n",
    "output_path = os.path.join(data_path,'processed_nvt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075c7dd8-fdf0-425c-8633-f42b2b93da6d",
   "metadata": {},
   "source": [
    "Read raw parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83670e06-5f66-4efe-938e-45f044488814",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/cudf/core/frame.py:384: UserWarning: The deep parameter is ignored and is only included for pandas compatibility.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_dataset = nvt.Dataset(os.path.join(data_path, 'train.parquet'))\n",
    "valid_dataset = nvt.Dataset(os.path.join(data_path, 'valid.parquet'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe50803-8b2f-49bc-840f-8fa97c317318",
   "metadata": {},
   "source": [
    "Let's print out a few rows from our training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6acfabf-374f-4ea4-a144-91fae6ea79b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>event_time</th>\n",
       "      <th>event_type</th>\n",
       "      <th>brand</th>\n",
       "      <th>price</th>\n",
       "      <th>user_session</th>\n",
       "      <th>target</th>\n",
       "      <th>cat_0</th>\n",
       "      <th>cat_1</th>\n",
       "      <th>cat_2</th>\n",
       "      <th>cat_3</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>ts_month</th>\n",
       "      <th>event_time_ts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>635096898</td>\n",
       "      <td>26205398</td>\n",
       "      <td>2020-03-31 20:00:17 UTC</td>\n",
       "      <td>purchase</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>178.380005</td>\n",
       "      <td>27282c23-cf25-436f-87f9-b1fefa8ecee3</td>\n",
       "      <td>1</td>\n",
       "      <td>construction</td>\n",
       "      <td>components</td>\n",
       "      <td>faucet</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2020-03-31 20:00:17</td>\n",
       "      <td>3</td>\n",
       "      <td>1585684817000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>635096898</td>\n",
       "      <td>26205378</td>\n",
       "      <td>2020-03-31 19:58:21 UTC</td>\n",
       "      <td>purchase</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>263.070007</td>\n",
       "      <td>27282c23-cf25-436f-87f9-b1fefa8ecee3</td>\n",
       "      <td>1</td>\n",
       "      <td>construction</td>\n",
       "      <td>components</td>\n",
       "      <td>faucet</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2020-03-31 19:58:21</td>\n",
       "      <td>3</td>\n",
       "      <td>1585684701000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>635096898</td>\n",
       "      <td>26500131</td>\n",
       "      <td>2020-03-31 19:55:25 UTC</td>\n",
       "      <td>purchase</td>\n",
       "      <td>lucente</td>\n",
       "      <td>194.820007</td>\n",
       "      <td>27282c23-cf25-436f-87f9-b1fefa8ecee3</td>\n",
       "      <td>1</td>\n",
       "      <td>kids</td>\n",
       "      <td>toys</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2020-03-31 19:55:25</td>\n",
       "      <td>3</td>\n",
       "      <td>1585684525000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>635096898</td>\n",
       "      <td>26500571</td>\n",
       "      <td>2020-03-31 19:47:51 UTC</td>\n",
       "      <td>cart</td>\n",
       "      <td>lucente</td>\n",
       "      <td>205.580002</td>\n",
       "      <td>27282c23-cf25-436f-87f9-b1fefa8ecee3</td>\n",
       "      <td>0</td>\n",
       "      <td>kids</td>\n",
       "      <td>toys</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2020-03-31 19:47:51</td>\n",
       "      <td>3</td>\n",
       "      <td>1585684071000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>635096898</td>\n",
       "      <td>26500149</td>\n",
       "      <td>2020-03-31 19:46:45 UTC</td>\n",
       "      <td>purchase</td>\n",
       "      <td>lucente</td>\n",
       "      <td>309.809998</td>\n",
       "      <td>27282c23-cf25-436f-87f9-b1fefa8ecee3</td>\n",
       "      <td>1</td>\n",
       "      <td>kids</td>\n",
       "      <td>toys</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2020-03-31 19:46:45</td>\n",
       "      <td>3</td>\n",
       "      <td>1585684005000000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     user_id  product_id               event_time event_type    brand  \\\n",
       "0  635096898    26205398  2020-03-31 20:00:17 UTC   purchase     <NA>   \n",
       "1  635096898    26205378  2020-03-31 19:58:21 UTC   purchase     <NA>   \n",
       "2  635096898    26500131  2020-03-31 19:55:25 UTC   purchase  lucente   \n",
       "3  635096898    26500571  2020-03-31 19:47:51 UTC       cart  lucente   \n",
       "4  635096898    26500149  2020-03-31 19:46:45 UTC   purchase  lucente   \n",
       "\n",
       "        price                          user_session  target         cat_0  \\\n",
       "0  178.380005  27282c23-cf25-436f-87f9-b1fefa8ecee3       1  construction   \n",
       "1  263.070007  27282c23-cf25-436f-87f9-b1fefa8ecee3       1  construction   \n",
       "2  194.820007  27282c23-cf25-436f-87f9-b1fefa8ecee3       1          kids   \n",
       "3  205.580002  27282c23-cf25-436f-87f9-b1fefa8ecee3       0          kids   \n",
       "4  309.809998  27282c23-cf25-436f-87f9-b1fefa8ecee3       1          kids   \n",
       "\n",
       "        cat_1   cat_2 cat_3           timestamp  ts_month        event_time_ts  \n",
       "0  components  faucet  <NA> 2020-03-31 20:00:17         3  1585684817000000000  \n",
       "1  components  faucet  <NA> 2020-03-31 19:58:21         3  1585684701000000000  \n",
       "2        toys    <NA>  <NA> 2020-03-31 19:55:25         3  1585684525000000000  \n",
       "3        toys    <NA>  <NA> 2020-03-31 19:47:51         3  1585684071000000000  \n",
       "4        toys    <NA>  <NA> 2020-03-31 19:46:45         3  1585684005000000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.to_ddf().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e0709c-4107-401e-992e-74a4f0ac5c03",
   "metadata": {},
   "source": [
    "Categorify categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4ca3533-399a-4783-894d-88a39044c2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = [\"user_id\"] >> Categorify(dtype='int32') >> TagAsUserID()\n",
    "\n",
    "item_id = [\"product_id\"] >> Categorify(dtype='int32') >> TagAsItemID()\n",
    "\n",
    "item_features = [\"cat_0\", \"cat_1\", \"cat_2\", \"brand\"] >> Categorify(dtype='int32') >> TagAsItemFeatures()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce38810-23f1-4e14-9728-28864e6e79ad",
   "metadata": {},
   "source": [
    "Create temporal features and categorify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5124e282-39b9-4ce9-aedf-a4be57904b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday = (\n",
    "    [\"timestamp\"] >> \n",
    "    LambdaOp(lambda col: col.dt.weekday) >> \n",
    "    Rename(name ='ts_weekday')\n",
    ")\n",
    "\n",
    "hour = (\n",
    "    [\"timestamp\"] >> \n",
    "    LambdaOp(lambda col: col.dt.hour) >> \n",
    "    Rename(name ='ts_hour')\n",
    ")\n",
    "\n",
    "timestamp = ['event_time_ts'] >> nvt.ops.AddMetadata(tags=[Tags.TIME]) \n",
    "\n",
    "context_features = (\n",
    "    (weekday + hour)  \n",
    "    >> Categorify(dtype='int32') >> TagAsUserFeatures()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f922c936-7e0c-48e0-85e3-15611c752ef2",
   "metadata": {},
   "source": [
    "Apply a user defined function to calculate relative price to the average price for the product_id using `LambdaOp` and transform continuous features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b90abd4-334a-499a-a5ef-3dbe48e2ec5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relative price to the average price for the product_id\n",
    "def relative_price_to_avg_pr(col, gdf):\n",
    "    epsilon = 1e-5\n",
    "    col = ((gdf['price'] - col) / (col + epsilon)) * (col > 0).astype(int)\n",
    "    return col\n",
    "\n",
    "\n",
    "price = (\n",
    "    ['price']\n",
    "    >> FillMissing(0)\n",
    "    >> LogOp()\n",
    "    >> Normalize()\n",
    "    >> LambdaOp(lambda col: col.astype(\"float32\"))\n",
    "    >> TagAsItemFeatures()\n",
    ")   \n",
    "\n",
    "avg_price_product = ['product_id'] >> JoinGroupby(cont_cols =['price'], stats=[\"mean\"])\n",
    "\n",
    "relative_price_to_avg = (\n",
    "    avg_price_product \n",
    "    >> LambdaOp(relative_price_to_avg_pr, dependency=['price']) \n",
    "    >> LambdaOp(lambda col: col.astype(\"float32\"))\n",
    "    >> Rename(name='relative_price')\n",
    "    >> AddMetadata(tags=[\"item\", Tags.CONTINUOUS])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58f7af1-0df5-414e-b098-2b1940def0ed",
   "metadata": {},
   "source": [
    "Below, we apply target encoding (TE) to categorical columns. `TE` is a popular feature engineering technique for tabular data. `TE` calculates the statistics from a target variable grouped by the unique values of one or more categorical features. It was used by many top solutions in the [RecSys2020](https://blog.twitter.com/engineering/en_us/topics/insights/2020/what_twitter_learned_from_recsys2020) competition. In previous years, TE was used, as well. You can read and lear more about target encoding in this [blog post](https://medium.com/rapids-ai/target-encoding-with-rapids-cuml-do-more-with-your-categorical-data-8c762c79e784)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38ce1181-ec5b-47c8-b353-356ee84d17aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_groups =  nvt.ColumnSelector(['user_id', 'brand', 'cat_1', 'cat_2'])\n",
    "label = nvt.ColumnSelector([\"target\"])\n",
    "te_features = cat_groups >> TargetEncoding(label)\n",
    "te_features_norm = te_features >> Normalize() >> LambdaOp(lambda col: col.astype(\"float32\")) >> TagAsItemFeatures()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7e122c-ba06-494e-bf41-d77e28371440",
   "metadata": {},
   "source": [
    "Tag target column and original raw user_id and item_id columns. We keep raw user_id and item_id because in the next notebook we will register them to feature store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c68873e-7d35-4db7-8667-eea459e40afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_raw = [\"user_id\"] >> Rename(postfix='_raw') >> TagAsUserFeatures()\n",
    "item_id_raw = [\"product_id\"] >> Rename(postfix='_raw') >> TagAsItemFeatures()\n",
    "\n",
    "target = (\n",
    "    [\"target\"] \n",
    "    >> nvt.ops.AddMetadata(tags=[Tags.BINARY_CLASSIFICATION, \"target\"]) \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fcbab9-c390-4e67-8af9-5a9bf982b1e8",
   "metadata": {},
   "source": [
    "Create workflow output node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4400cce-3e33-48ae-ab48-dd83a8e11fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = (user_id  + \n",
    "           context_features + \n",
    "           item_id + \n",
    "           item_features + \n",
    "           price + \n",
    "           relative_price_to_avg + \n",
    "           te_features_norm + \n",
    "           timestamp +\n",
    "           user_id_raw +\n",
    "           item_id_raw +\n",
    "           target\n",
    "          )\n",
    "workflow = nvt.Workflow(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1e9f458-1ba2-46e1-b8d5-34143f72ab87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:148: UserWarning: Compound tags like Tags.USER_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.USER: 'user'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:148: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/cudf/core/frame.py:384: UserWarning: The deep parameter is ignored and is only included for pandas compatibility.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.72 s, sys: 2.54 s, total: 7.26 s\n",
      "Wall time: 18.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "workflow.fit(train_dataset)\n",
    "\n",
    "workflow.transform(train_dataset).to_parquet(\n",
    "    output_path=os.path.join(output_path, \"train\")\n",
    ")\n",
    "\n",
    "workflow.transform(valid_dataset).to_parquet(\n",
    "    output_path=os.path.join(output_path, \"valid\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d0b520-9c6a-4634-a2b3-1f29e81334a9",
   "metadata": {},
   "source": [
    "We can save the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00c5bbd4-bce8-4318-975b-d925e853947e",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.save(os.path.join(output_path, \"workflow\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53260d1f-9ece-48eb-8f15-7617cbb3141e",
   "metadata": {},
   "source": [
    "NVTabular exported the schema file of our processed dataset. The schema.pbtxt is a protobuf text file contains features metadata, including statistics about features such as cardinality, min and max values and also tags based on their characteristics and dtypes (e.g., categorical, continuous, list, item_id). The metadata information is loaded from schema and their tags are used to automatically set the parameters of Merlin Models. In other words, Merlin Models relies on the schema object to automatically build all necessary input and output layers.\n",
    "\n",
    "To learn more about NVTabular and schema object you can visit the example notebooks in the NVTabular [repo](https://github.com/NVIDIA-Merlin/NVTabular/tree/main/examples) and Merlin Models [repo](https://github.com/NVIDIA-Merlin/models/blob/main/examples/02-Merlin-Models-and-NVTabular-integration.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cb2cad-b4eb-44e0-b185-6fdf2aa45e93",
   "metadata": {},
   "source": [
    "### 2.2. Model Building and Training on GPU with Merlin Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795c69bd-8fdf-4324-b0b5-d47b2409def5",
   "metadata": {},
   "source": [
    "**GOAL:** In this lab, we build ranking models for a binary classification task, which aims to predict the likelihood (a relevance score) of a product to be purchased by a given user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372a3a0c-ba22-4efe-b32b-58b3833cfabd",
   "metadata": {},
   "source": [
    "**Read processed parquet files as Dataset objects**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1a8f783-91e9-4890-801b-46adfe504d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = Dataset(os.path.join(output_path, \"train\", \"*.parquet\"), part_size=\"500MB\")\n",
    "valid = Dataset(os.path.join(output_path, \"valid\", \"*.parquet\"), part_size=\"500MB\")\n",
    "\n",
    "# define schema object\n",
    "schema = train.schema.without(['event_time_ts', 'user_id_raw', 'product_id_raw'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9130a100-8629-4688-929b-e6104a0bf63f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>tags</th>\n",
       "      <th>dtype</th>\n",
       "      <th>is_list</th>\n",
       "      <th>is_ragged</th>\n",
       "      <th>properties.num_buckets</th>\n",
       "      <th>properties.freq_threshold</th>\n",
       "      <th>properties.max_size</th>\n",
       "      <th>properties.start_index</th>\n",
       "      <th>properties.cat_path</th>\n",
       "      <th>properties.embedding_sizes.cardinality</th>\n",
       "      <th>properties.embedding_sizes.dimension</th>\n",
       "      <th>properties.domain.min</th>\n",
       "      <th>properties.domain.max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>user_id</td>\n",
       "      <td>(Tags.CATEGORICAL, Tags.USER, Tags.ID, Tags.US...</td>\n",
       "      <td>int32</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>.//categories/unique.user_id.parquet</td>\n",
       "      <td>350630.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>350629.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ts_weekday</td>\n",
       "      <td>(Tags.CATEGORICAL, Tags.USER)</td>\n",
       "      <td>int32</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>.//categories/unique.ts_weekday.parquet</td>\n",
       "      <td>8.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ts_hour</td>\n",
       "      <td>(Tags.CATEGORICAL, Tags.USER)</td>\n",
       "      <td>int32</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>.//categories/unique.ts_hour.parquet</td>\n",
       "      <td>25.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>product_id</td>\n",
       "      <td>(Tags.CATEGORICAL, Tags.ITEM_ID, Tags.ID, Tags...</td>\n",
       "      <td>int32</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>.//categories/unique.product_id.parquet</td>\n",
       "      <td>51376.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51375.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cat_0</td>\n",
       "      <td>(Tags.CATEGORICAL, Tags.ITEM)</td>\n",
       "      <td>int32</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>.//categories/unique.cat_0.parquet</td>\n",
       "      <td>14.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cat_1</td>\n",
       "      <td>(Tags.CATEGORICAL, Tags.ITEM)</td>\n",
       "      <td>int32</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>.//categories/unique.cat_1.parquet</td>\n",
       "      <td>61.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cat_2</td>\n",
       "      <td>(Tags.CATEGORICAL, Tags.ITEM)</td>\n",
       "      <td>int32</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>.//categories/unique.cat_2.parquet</td>\n",
       "      <td>90.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>brand</td>\n",
       "      <td>(Tags.CATEGORICAL, Tags.ITEM)</td>\n",
       "      <td>int32</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>.//categories/unique.brand.parquet</td>\n",
       "      <td>2653.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2652.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>price</td>\n",
       "      <td>(Tags.CONTINUOUS, Tags.ITEM)</td>\n",
       "      <td>float32</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>relative_price</td>\n",
       "      <td>(Tags.CONTINUOUS, Tags.ITEM)</td>\n",
       "      <td>float32</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>TE_user_id_target</td>\n",
       "      <td>(Tags.CONTINUOUS, Tags.ITEM)</td>\n",
       "      <td>float32</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>TE_brand_target</td>\n",
       "      <td>(Tags.CONTINUOUS, Tags.ITEM)</td>\n",
       "      <td>float32</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>TE_cat_1_target</td>\n",
       "      <td>(Tags.CONTINUOUS, Tags.ITEM)</td>\n",
       "      <td>float32</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>TE_cat_2_target</td>\n",
       "      <td>(Tags.CONTINUOUS, Tags.ITEM)</td>\n",
       "      <td>float32</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>target</td>\n",
       "      <td>(Tags.BINARY_CLASSIFICATION, Tags.TARGET)</td>\n",
       "      <td>int32</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "[{'name': 'user_id', 'tags': {<Tags.CATEGORICAL: 'categorical'>, <Tags.USER: 'user'>, <Tags.ID: 'id'>, <Tags.USER_ID: 'user_id'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0.0, 'max_size': 0.0, 'start_index': 0.0, 'cat_path': './/categories/unique.user_id.parquet', 'embedding_sizes': {'cardinality': 350630.0, 'dimension': 512.0}, 'domain': {'min': 0, 'max': 350629}}, 'dtype': dtype('int32'), 'is_list': False, 'is_ragged': False}, {'name': 'ts_weekday', 'tags': {<Tags.CATEGORICAL: 'categorical'>, <Tags.USER: 'user'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0.0, 'max_size': 0.0, 'start_index': 0.0, 'cat_path': './/categories/unique.ts_weekday.parquet', 'embedding_sizes': {'cardinality': 8.0, 'dimension': 16.0}, 'domain': {'min': 0, 'max': 7}}, 'dtype': dtype('int32'), 'is_list': False, 'is_ragged': False}, {'name': 'ts_hour', 'tags': {<Tags.CATEGORICAL: 'categorical'>, <Tags.USER: 'user'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0.0, 'max_size': 0.0, 'start_index': 0.0, 'cat_path': './/categories/unique.ts_hour.parquet', 'embedding_sizes': {'cardinality': 25.0, 'dimension': 16.0}, 'domain': {'min': 0, 'max': 24}}, 'dtype': dtype('int32'), 'is_list': False, 'is_ragged': False}, {'name': 'product_id', 'tags': {<Tags.CATEGORICAL: 'categorical'>, <Tags.ITEM_ID: 'item_id'>, <Tags.ID: 'id'>, <Tags.ITEM: 'item'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0.0, 'max_size': 0.0, 'start_index': 0.0, 'cat_path': './/categories/unique.product_id.parquet', 'embedding_sizes': {'cardinality': 51376.0, 'dimension': 512.0}, 'domain': {'min': 0, 'max': 51375}}, 'dtype': dtype('int32'), 'is_list': False, 'is_ragged': False}, {'name': 'cat_0', 'tags': {<Tags.CATEGORICAL: 'categorical'>, <Tags.ITEM: 'item'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0.0, 'max_size': 0.0, 'start_index': 0.0, 'cat_path': './/categories/unique.cat_0.parquet', 'embedding_sizes': {'cardinality': 14.0, 'dimension': 16.0}, 'domain': {'min': 0, 'max': 13}}, 'dtype': dtype('int32'), 'is_list': False, 'is_ragged': False}, {'name': 'cat_1', 'tags': {<Tags.CATEGORICAL: 'categorical'>, <Tags.ITEM: 'item'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0.0, 'max_size': 0.0, 'start_index': 0.0, 'cat_path': './/categories/unique.cat_1.parquet', 'embedding_sizes': {'cardinality': 61.0, 'dimension': 16.0}, 'domain': {'min': 0, 'max': 60}}, 'dtype': dtype('int32'), 'is_list': False, 'is_ragged': False}, {'name': 'cat_2', 'tags': {<Tags.CATEGORICAL: 'categorical'>, <Tags.ITEM: 'item'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0.0, 'max_size': 0.0, 'start_index': 0.0, 'cat_path': './/categories/unique.cat_2.parquet', 'embedding_sizes': {'cardinality': 90.0, 'dimension': 20.0}, 'domain': {'min': 0, 'max': 89}}, 'dtype': dtype('int32'), 'is_list': False, 'is_ragged': False}, {'name': 'brand', 'tags': {<Tags.CATEGORICAL: 'categorical'>, <Tags.ITEM: 'item'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0.0, 'max_size': 0.0, 'start_index': 0.0, 'cat_path': './/categories/unique.brand.parquet', 'embedding_sizes': {'cardinality': 2653.0, 'dimension': 132.0}, 'domain': {'min': 0, 'max': 2652}}, 'dtype': dtype('int32'), 'is_list': False, 'is_ragged': False}, {'name': 'price', 'tags': {<Tags.CONTINUOUS: 'continuous'>, <Tags.ITEM: 'item'>}, 'properties': {}, 'dtype': dtype('float32'), 'is_list': False, 'is_ragged': False}, {'name': 'relative_price', 'tags': {<Tags.CONTINUOUS: 'continuous'>, <Tags.ITEM: 'item'>}, 'properties': {}, 'dtype': dtype('float32'), 'is_list': False, 'is_ragged': False}, {'name': 'TE_user_id_target', 'tags': {<Tags.CONTINUOUS: 'continuous'>, <Tags.ITEM: 'item'>}, 'properties': {}, 'dtype': dtype('float32'), 'is_list': False, 'is_ragged': False}, {'name': 'TE_brand_target', 'tags': {<Tags.CONTINUOUS: 'continuous'>, <Tags.ITEM: 'item'>}, 'properties': {}, 'dtype': dtype('float32'), 'is_list': False, 'is_ragged': False}, {'name': 'TE_cat_1_target', 'tags': {<Tags.CONTINUOUS: 'continuous'>, <Tags.ITEM: 'item'>}, 'properties': {}, 'dtype': dtype('float32'), 'is_list': False, 'is_ragged': False}, {'name': 'TE_cat_2_target', 'tags': {<Tags.CONTINUOUS: 'continuous'>, <Tags.ITEM: 'item'>}, 'properties': {}, 'dtype': dtype('float32'), 'is_list': False, 'is_ragged': False}, {'name': 'target', 'tags': {<Tags.BINARY_CLASSIFICATION: 'binary_classification'>, <Tags.TARGET: 'target'>}, 'properties': {}, 'dtype': dtype('int32'), 'is_list': False, 'is_ragged': False}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68f54857-5c5e-4720-aa4b-dff9f81ef3a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'target'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_column = schema.select_by_tag(Tags.TARGET).column_names[0]\n",
    "target_column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4d14c7-8013-4f97-8721-bf51ffc2e73b",
   "metadata": {},
   "source": [
    "#### 2.2.1. DLRM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1889a80-4cc4-4b81-a81d-9311c70495b2",
   "metadata": {},
   "source": [
    "Deep Learning Recommendation Model (DLRM) architecture is a popular neural network model originally proposed by Facebook in 2019 as a personalization deep learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c23232-2732-492a-bce1-7b4e9672209a",
   "metadata": {},
   "source": [
    "<img src=\"./images/DLRM.png\" width=\"600\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80af7f3-f22a-41eb-a58b-a907b6438f71",
   "metadata": {},
   "source": [
    "DLRM accepts two types of features: categorical and numerical.\n",
    "\n",
    "- For each categorical feature, an embedding table is used to provide dense representation to each unique value.\n",
    "- For numerical features, they are fed to model as dense features, and then transformed by a simple neural network referred to as \"bottom MLP\". This part of the network consists of a series of linear layers with ReLU activations.\n",
    "- The output of the bottom MLP and the embedding vectors are then fed into the dot product interaction operation (see Pairwise interaction step). The output of \"dot interaction\" is then concatenated with the features resulting from the bottom MLP (we apply a skip-connection there) and fed into the \"top MLP\" which is also a series of dense layers with activations ((a fully connected NN).\n",
    "- The model outputs a single number (here we use sigmoid function to generate probabilities) which can be interpreted as a likelihood of a certain user clicking on an ad, watching a movie, or viewing a news page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "416954ba-1f43-40ed-b3a5-d8003d0df756",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mm.DLRMModel(\n",
    "    schema,\n",
    "    embedding_dim=64,\n",
    "    bottom_block=mm.MLPBlock([128, 64]),\n",
    "    top_block=mm.MLPBlock([128, 64, 32]),\n",
    "    prediction_tasks=mm.BinaryClassificationTask(target_column),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d379252-5cb9-45ed-a2fb-6c804a2a7d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "735/735 [==============================] - 21s 16ms/step - loss: 0.5772 - auc: 0.7655 - regularization_loss: 0.0000e+00 - val_loss: 0.7187 - val_auc: 0.6390 - val_regularization_loss: 0.0000e+00\n",
      "Epoch 2/2\n",
      "735/735 [==============================] - 11s 14ms/step - loss: 0.4113 - auc: 0.8925 - regularization_loss: 0.0000e+00 - val_loss: 0.7963 - val_auc: 0.6286 - val_regularization_loss: 0.0000e+00\n",
      "CPU times: user 46.1 s, sys: 7.63 s, total: 53.7 s\n",
      "Wall time: 34 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4016079280>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "model.compile(optimizer='adam', run_eagerly=False, metrics=[tf.keras.metrics.AUC()])\n",
    "model.fit(train, validation_data=valid, batch_size=4096, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806d989e-260a-4117-8536-28b3baee7ccd",
   "metadata": {},
   "source": [
    "#### 2.2.2. DCN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728c1919-39e1-4e2d-adc4-e612f6aa8046",
   "metadata": {},
   "source": [
    "DCN-V2 is an architecture proposed as an improvement upon the original [DCN model](https://arxiv.org/pdf/1708.05123.pdf). The explicit feature interactions of the inputs are learned through cross layers, and then combined with a deep network to learn complementary implicit interactions. The overall model architecture is depicted in Figure below, with two ways to combine the cross network with the deep network: (1) stacked and (2) parallel. The output of the embbedding layer is the concatenation of all the embedded vectors and the normalized dense features: x<sub>0</sub> = [x<sub>embed,1</sub>; . . . ; x<sub>embed,𝑛</sub>; 𝑥<sub>dense</sub>].\n",
    "\n",
    "![DCN](./images/DCN.png)\n",
    "\n",
    "<a href=\"https://arxiv.org/abs/2008.13535\">Image Source: DCN V2 paper</a>\n",
    "\n",
    "In this example, we build `DCN-v2 stacked` architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb1035e7-6306-4cc2-acb0-a06e005c2eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mm.DCNModel(\n",
    "    schema,\n",
    "    depth=2,\n",
    "    deep_block=mm.MLPBlock([64, 32]),\n",
    "    prediction_tasks=mm.BinaryClassificationTask(target_column),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b543fd5-e7cd-46c4-b786-ac9b22db981a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "  4/735 [..............................] - ETA: 14s - loss: 0.7026 - auc_1: 0.3903 - regularization_loss: 0.0000e+00    WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0089s vs `on_train_batch_end` time: 0.0097s). Check your callbacks.\n",
      "735/735 [==============================] - 17s 17ms/step - loss: 0.5724 - auc_1: 0.7709 - regularization_loss: 0.0000e+00 - val_loss: 0.7835 - val_auc_1: 0.6404 - val_regularization_loss: 0.0000e+00\n",
      "Epoch 2/2\n",
      "735/735 [==============================] - 11s 15ms/step - loss: 0.4624 - auc_1: 0.8611 - regularization_loss: 0.0000e+00 - val_loss: 0.9539 - val_auc_1: 0.6379 - val_regularization_loss: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4005e80790>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile('adam', run_eagerly=False, metrics=[tf.keras.metrics.AUC()])\n",
    "model.fit(train, validation_data=valid, batch_size=4096, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3af85b9-5b83-42b6-9cde-84870a2dfad7",
   "metadata": {},
   "source": [
    "**Injecting additional Hyper-parameters to DCN Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd862df-270f-4baf-98de-471668180bda",
   "metadata": {},
   "source": [
    "Hyperparameter tuning (optimization) is an important phenomenon to find the possible best sets of hyperparameters to build and train the model from a given dataset. Hyperparameter tuning can be done manually or be managed by an algorithm like grid search and bayesian optimization. The latter optimizes the search for the best hyperparameters guided by a metric that needs to be maximized or minimized.\n",
    "\n",
    "Below, we showcase how we can inject certain hyperparameters to our DCN model and set their values. Let's first give some explanation about these hyperparameters:\n",
    "\n",
    "In Merlin Models, we introduce a data class called, `EmdeddingOptions`, that  contains different parameters to configure the embedding tables of categorical variables. Among them are the embedding dimension, a boolean flag to set the optimal dimension inferred from the variable cardinality, and the embeddings' l2 regularization. `embedding_dims` controls the dimensionality of embeddings that will be created for categorical columns. Below, we use this argument to set embedding dimensions for `item_ids` and `user_ids` columns. The higher the value, the greater capacity our model will have. But greater capacity beyond a certain point might result in overfitting. Picking a good embedding size can go a long way and is best arrived at by experimentation. A related parameter here is `embeddings_l2_reg`. The higher the value, the greater the constraint put on the variability of values in our embeddings. This is another parameter that we can use to control the capacity of our model.\n",
    "\n",
    "\n",
    "In the [DCNModel](https://github.com/NVIDIA-Merlin/models/blob/main/merlin/models/tf/models/ranking.py#L87) constructor, the `depth` parameter specifies the number of cross-layers to be stacked. The default value is 1, and going above again increases the capacity of the model and makes the mappings that it can learn more expressive.\n",
    "\n",
    "The `deep_block` parameter is a Multilayer Perceptron block consisting of a stack of linear layers. Here the parameters we can alter are the number and dimensionality of layers (`[64, 32]` that is passed indicates 64 nodes in the first layer and 32 in the second), the activation of our layers controlled by the `activation` parameter.\n",
    "\n",
    "We again have two parameters for specifying the level of regularization:`kernel_regularizer` and `bias_regularizer` that allow us to apply penalties on layer parameters or layer activity during optimization.`kernel_regularizer` is used to apply a penalty on the layer's weights, whereas `bias_regularizer` is used to apply a penalty on the layer's bias. You can learn more about tf.keras.regularizers [here](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/Regularizer).\n",
    "\n",
    "The `dropout` parameter controls the amount of dropout that will be applied. With a value greater than 0, during training, some nodes (with probability equal to the `dropout` parameter value) will be excluded from the calculation. This is helpful as it can prevent overspecialization -- no node can be completely sure it will receive input from any other node in the forward pass. This parameter can be thought of as another way to combat overfitting.\n",
    "\n",
    "Last but not least, we can control the `epochs` parameter. Depending on other values that we specify, training for this or another number of epochs can lead to better results. We can use hyperparameter optimization to discover that value!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "efbb5608-3874-40da-9f00-f7867c28c377",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from merlin.models.utils import schema_utils\n",
    "from tensorflow.keras import regularizers\n",
    "embedding_dims = {}\n",
    "\n",
    "item_id_feature_name = schema.select_by_tag(Tags.ITEM_ID).column_names[0]\n",
    "item_id_domain = schema_utils.categorical_domains(schema)[\n",
    "    item_id_feature_name\n",
    "]\n",
    "embedding_dims[item_id_domain] = 128\n",
    "\n",
    "user_id_feature_name = schema.select_by_tag(Tags.USER_ID).column_names[0]\n",
    "user_id_domain = schema_utils.categorical_domains(schema)[\n",
    "    user_id_feature_name\n",
    "]\n",
    "embedding_dims[user_id_domain] = 128\n",
    "\n",
    "embedding_options = mm.EmbeddingOptions(\n",
    "    embedding_dims=embedding_dims,\n",
    "    infer_embedding_sizes=True,\n",
    "    embeddings_l2_reg=0.00\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37cecd18-c451-4e20-b183-4d0b99bb31e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mm.DCNModel(\n",
    "    schema,\n",
    "    depth=2,\n",
    "    deep_block=mm.MLPBlock(\n",
    "        [64, 32],\n",
    "        activation='selu',\n",
    "        no_activation_last_layer=False,\n",
    "        dropout=0.015,\n",
    "        kernel_regularizer=regularizers.l2(1e-5),\n",
    "        bias_regularizer=regularizers.l2(1e-5),\n",
    "    ),\n",
    "    stacked=True,\n",
    "    embedding_options=embedding_options,\n",
    "    prediction_tasks=mm.BinaryClassificationTask('target'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3607350-f53c-4e4b-b402-2d631daf08c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "735/735 [==============================] - 19s 20ms/step - loss: 0.5834 - auc_2: 0.7611 - regularization_loss: 0.0014 - val_loss: 0.7215 - val_auc_2: 0.6418 - val_regularization_loss: 0.0014\n",
      "Epoch 2/2\n",
      "735/735 [==============================] - 14s 19ms/step - loss: 0.4867 - auc_2: 0.8453 - regularization_loss: 0.0015 - val_loss: 0.7926 - val_auc_2: 0.6406 - val_regularization_loss: 0.0015\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f40054ef2b0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile('adam', run_eagerly=False, metrics=[tf.keras.metrics.AUC()])\n",
    "model.fit(train, validation_data=valid, batch_size=4096, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450fe1ab-2e56-4378-b65f-e2ff1a48b3ea",
   "metadata": {},
   "source": [
    "#### 2.2.3. XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0bad8c-d55f-498c-9267-566ed00627e8",
   "metadata": {},
   "source": [
    "[XGBoost](https://xgboost.ai/), which stands for Extreme Gradient Boosting, is a scalable, distributed gradient-boosted decision tree (GBDT) machine learning library. It provides parallel tree boosting and is the leading machine learning library for regression, classification, and ranking problems.\n",
    "\n",
    "A Gradient Boosting Decision Trees (GBDT) is a decision tree ensemble learning algorithm similar to random forest, for classification and regression. Ensemble learning algorithms combine multiple machine learning algorithms to obtain a better model. Both random forest and GBDT build a model consisting of multiple decision trees. The difference is in how the trees are built and combined.\n",
    "\n",
    "The term “gradient boosting” comes from the idea of “boosting” or improving a single weak model by combining it with a number of other weak models in order to generate a collectively strong model. Gradient boosting is an extension of boosting where the process of additively generating weak models is formalized as a gradient descent algorithm over an objective function. `XGBoost` is a scalable and highly accurate implementation of gradient boosting that pushes the limits of computing power for boosted tree algorithms, being built largely for energizing machine learning model performance and computational speed. With XGBoost, trees are built in parallel, instead of sequentially like GBDT. You can read more about XGBoost [here](https://www.nvidia.com/en-us/glossary/data-science/xgboost/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415865b9-52c7-44c4-9a41-97fc2a423d53",
   "metadata": {},
   "source": [
    "In order to facilitate training on data larger than the available GPU memory, the training will leverage Dask. All the complexity of starting a local dask cluster is hidden in the Distributed context manager.\n",
    "\n",
    "Without further ado, let's build and train our XGB model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "04fa0710-c0b9-49fc-b249-e1a5581ca1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-09 15:44:34,315 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n",
      "Failed to bind address 'None', trying to use '127.0.0.1' instead.\n",
      "[15:44:39] task [xgboost.dask]:tcp://127.0.0.1:36865 got new rank 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_set-auc:0.63497\n",
      "[10]\tvalidation_set-auc:0.63740\n",
      "[20]\tvalidation_set-auc:0.63688\n",
      "[30]\tvalidation_set-auc:0.63625\n",
      "[40]\tvalidation_set-auc:0.63632\n",
      "[50]\tvalidation_set-auc:0.63618\n",
      "[60]\tvalidation_set-auc:0.63623\n",
      "[70]\tvalidation_set-auc:0.63621\n",
      "[80]\tvalidation_set-auc:0.63642\n",
      "[90]\tvalidation_set-auc:0.63650\n",
      "[99]\tvalidation_set-auc:0.63650\n"
     ]
    }
   ],
   "source": [
    "from merlin.core.utils import Distributed\n",
    "from merlin.models.xgb import XGBoost\n",
    "xgb_booster_params = {\n",
    "    'objective':'binary:logistic',\n",
    "    'tree_method':'gpu_hist',\n",
    "    'eval_metric': \"auc\"\n",
    "}\n",
    "\n",
    "xgb_train_params = {\n",
    "    'num_boost_round': 100,\n",
    "    'verbose_eval': 10,\n",
    "}\n",
    "\n",
    "\n",
    "with Distributed():\n",
    "    model = XGBoost(schema=schema, **xgb_booster_params)\n",
    "    model.fit(\n",
    "        train,\n",
    "        evals=[(valid, 'validation_set'),],\n",
    "        use_quantile = False,\n",
    "        **xgb_train_params\n",
    "    )\n",
    "    metrics = model.evaluate(valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f66cb4-d4d5-42c0-8f49-c2227e4171df",
   "metadata": {},
   "source": [
    "Print eval metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30beabfc-446b-4444-aec2-35d254fb6ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auc': 0.6364974402143888}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c502f0-1c0c-4e58-9801-a9b296dba374",
   "metadata": {},
   "source": [
    "### Summary "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e94162-fd41-477b-ab10-f2e603dc6bd2",
   "metadata": {},
   "source": [
    "In this hands-on lab we learned how\n",
    "- to do feature preprocessing and generation on GPU using NVTabular library\n",
    "- NVTabular and Merlin Models are seamlessly integrated via schema object\n",
    "- build and train popular Recommender model architectures easily with Merlin Models library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8e7d48-1c62-4e94-a22e-6ee6447c4ed3",
   "metadata": {},
   "source": [
    "Please execute the cell below to shut down the kernel before moving on to the next notebook `03-Customize-Merlin-Models`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5045d70c-61c6-4b60-8065-2da96ba7c6d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
