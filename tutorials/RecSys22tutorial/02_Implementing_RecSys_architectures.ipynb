{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f62c3c8c-b14e-4933-8b70-fc5a833f2e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ====="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4cca29-4c9e-411d-8aa8-dd10b7d7aa00",
   "metadata": {},
   "source": [
    "# Training and Deploying Multi-Stage Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc0dc90-36d5-4789-88b0-329bda426427",
   "metadata": {
    "tags": []
   },
   "source": [
    "Industrial recommender systems are made up of complex pipelines requiring multiple steps including feature engineering and\n",
    "preprocessing, a retrieval model for candidate generation, filtering, a feature store query, a ranking model for scoring, and an ordering\n",
    "stage. These pipelines need to be carefully deployed as a set, requiring coordination during their development and deployment. Data\n",
    "scientists, ML engineers, and researchers might focus on different stages of recommender systems, however they share a common\n",
    "desire to reduce the time and effort searching for and combining boilerplate code coming from different sources or writing custom\n",
    "code from scratch to create their own RecSys pipelines.\n",
    "\n",
    "This tutorial introduces the Merlin framework which aims to make the development and deployment of recommender systems\n",
    "easier, providing methods for evaluating existing approaches, developing new ideas and deploying them to production. There are\n",
    "many techniques, such as different model architectures (e.g. MF, DLRM, DCN, etc), negative sampling strategies, loss functions or\n",
    "prediction tasks (binary, multi-class, multi-task) that are commonly used in these pipelines. Merlin provides building blocks that allow\n",
    "RecSys practitioners to focus on the ‚Äúwhat‚Äù question in designing their model pipeline instead of ‚Äúhow‚Äù. Supporting research into new\n",
    "ideas within the RecSys spaces is equally important and Merlin supports the addition of custom components and the extension of\n",
    "existing ones to address gaps.\n",
    "\n",
    "In this tutorial, participants will learn: \n",
    "   - (i) how to easily implement common recommender system techniques for comparison, \n",
    "   - (ii) how to modify components to evaluate new ideas,\n",
    "   - (iii) deploying recommender systems, bringing new ideas to production- using an open source framework Merlin and its libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbbc0ed-0253-4f56-a368-a5536c77e8e9",
   "metadata": {},
   "source": [
    "## 1. Implementing popular RecSys architectures and algorithms with Merlin Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbe7611-8697-487d-983c-a8ec0187f773",
   "metadata": {},
   "source": [
    "**Learning Objectives of this lab**\n",
    "\n",
    "- Introduction to the open source framework Merlin and its libraries- NVTabular and Merlin Models\n",
    "- Pre-processing and feature engineering with NVTabular\n",
    "- Build and train common recommender systems models with Merlin Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d509fb0f-c373-40ae-9dc7-799db33ed2d5",
   "metadata": {},
   "source": [
    "### NVIDIA Merlin\n",
    "\n",
    "Merlin is an open-source framework for building large-scale (deep learning) recommender systems. It is designed to support recommender systems end-to-end from ETL to training to deployment on CPU or GPU. Common deep learning frameworks are integrated such as TensorFlow or PyTorch. Its key benefits are the easy-to-use APIs, accelerations with GPU and scaling to multi-GPU or multi-node systems.\n",
    "\n",
    "![Merlin](./images/Merlin.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ba0a6a-f645-4d60-a404-fea022bac596",
   "metadata": {},
   "source": [
    "### Merlin Models\n",
    "\n",
    "[Merlin Models](https://github.com/NVIDIA-Merlin/models) is a library to make it easy for users in industry or academia to train and deploy recommender models with best practices baked into the library. This will let users in industry easily train standard models against their own dataset, getting high performance GPU accelerated models into production. This will also let researchers to build custom models by incorporating standard components of deep learning recommender models, and then benchmark their new models on example offline datasets.Core features are:\n",
    "- Unified API enables users to create models in TensorFlow or PyTorch\n",
    "- Deep integration with NVTabular for ETL and model serving\n",
    "- Flexible APIs targeted to both production and research\n",
    "- Many different recommender system architectures (tabular, two-tower, sequential) or tasks (binary, multi-class classification, multi-task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739671d0-8799-4803-9b09-aac6028a0786",
   "metadata": {},
   "source": [
    "### NVTabular "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e483dd-0388-4fd8-9047-6782ab0a2af4",
   "metadata": {},
   "source": [
    "[NVTabular](https://github.com/NVIDIA-Merlin/NVTabular) is a feature engineering and preprocessing library for tabular data that is designed to easily manipulate terabyte scale datasets and train deep learning (DL) based recommender systems. It provides high-level abstraction to simplify code and accelerates computation on the GPU using the RAPIDS Dask-cuDF library. NVTabular helps data scientists and ML engineers to:\n",
    "- process datasets that exceed GPU and CPU memory without having to worry about scale\n",
    "- focus on what to do with the data and not how to do it by using abstraction at the operation level\n",
    "- prepare datasets quickly and easily for experimentation so that more models can be trained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f32539f-a052-4d69-b6d5-17feeb9107a3",
   "metadata": {},
   "source": [
    "![Merlin](./images/schema.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c147d969-34c6-4cd4-87b3-d6f12a8d5aeb",
   "metadata": {},
   "source": [
    "That's a short introduction into Merlin, NVTabular and Merlin Models. If you are interested to learn more, we provide many examples in our GitHub repositories. \n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9d46aa-e234-4a7e-bed5-46bc0d7a4c21",
   "metadata": {},
   "source": [
    "### Feature Engineering on GPU with NVTabular"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6852d46-7d27-4ce2-b798-d8a9cfbd911e",
   "metadata": {},
   "source": [
    "In this notebook, we use publicly available [eCommerce](https://www.kaggle.com/mkechinov/ecommerce-behavior-data-from-multi-category-store) behavior dataset. The full dataset contains 7 months of data (from October 2019 to April 2020) from a large multi-category online store. Each row in the file represents an event. All events are related to products and users. Each event is like many-to-many relation between products and users. Data collected by Open CDP project and the source of the dataset is [REES46 Marketing Platform](https://rees46.com/). You can visit [Kaggle](https://www.kaggle.com/mkechinov/ecommerce-behavior-data-from-multi-category-store) to download the csv files. \n",
    "\n",
    "The csv files from 2019-Oct to 2020-April have been pre-processed in the `01_Introduction` notebook and we created train and valiation data sets to train and validate our models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be2fd7e-f454-41fa-b268-8e308ee5e473",
   "metadata": {},
   "source": [
    "In the feature engineering step, we do following data operations:\n",
    "\n",
    "- Categorify Categories\n",
    "- Create temporal features\n",
    "- Transform Continuous features\n",
    "- Count Encoding (JoinGroupBy)\n",
    "- Target Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b54d3c6-fe6c-4a41-ab5e-c80c92efd91d",
   "metadata": {},
   "source": [
    "**Import Required Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1423d7a5-1ec7-4c9b-a65d-4ac6216f9817",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-11 20:41:08.144752: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-11 20:41:09.295040: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 16249 MB memory:  -> device: 0, name: Quadro GV100, pci bus id: 0000:2d:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import glob\n",
    "import cudf \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nvtabular as nvt\n",
    "from nvtabular.ops import *\n",
    "import gc\n",
    "\n",
    "from merlin.schema.tags import Tags\n",
    "import merlin.models.tf as mm\n",
    "from merlin.io.dataset import Dataset\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8efaea78-8cc2-4483-a200-21fbb88f1125",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './'\n",
    "output_path = os.path.join(data_path,'processed_nvt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83670e06-5f66-4efe-938e-45f044488814",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/cudf/core/frame.py:384: UserWarning: The deep parameter is ignored and is only included for pandas compatibility.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_dataset = nvt.Dataset(os.path.join(data_path, 'train.parquet'))\n",
    "valid_dataset = nvt.Dataset(os.path.join(data_path, 'valid.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42b6b9af-edab-47b5-9122-6e9af17ff619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relative price to the average price for the product_id\n",
    "def relative_price_to_avg_categ(col, gdf):\n",
    "    epsilon = 1e-5\n",
    "    col = ((gdf['price'] - col) / (col + epsilon)) * (col > 0).astype(int)\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5124e282-39b9-4ce9-aedf-a4be57904b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = [\"user_id\"] >> Categorify() >> TagAsUserID()\n",
    "\n",
    "weekday = (\n",
    "    [\"timestamp\"] >> \n",
    "    LambdaOp(lambda col: col.dt.weekday) >> \n",
    "    Rename(name ='ts_weekday')\n",
    ")\n",
    "\n",
    "day = (\n",
    "    [\"timestamp\"] >> \n",
    "    LambdaOp(lambda col: col.dt.day) >> \n",
    "    Rename(name ='ts_day')\n",
    ")\n",
    "\n",
    "hour = (\n",
    "    [\"timestamp\"] >> \n",
    "    LambdaOp(lambda col: col.dt.hour) >> \n",
    "    Rename(name ='ts_hour')\n",
    ")\n",
    "\n",
    "context_features = (\n",
    "    (hour + weekday + day) \n",
    "    >> Categorify() >> TagAsUserFeatures()\n",
    ")\n",
    "\n",
    "# count encode `userId`\n",
    "count_logop_feat = (\n",
    "    [\"product_id\"]\n",
    "    >> JoinGroupby(cont_cols=[\"target\"], stats=[\"count\"], out_path='./categories_count')\n",
    "    >> LogOp()\n",
    "    >> TagAsUserFeatures()\n",
    ")\n",
    "\n",
    "\n",
    "item_id = [\"product_id\"] >> Categorify() >> TagAsItemID()\n",
    "\n",
    "item_features = [\"cat_0\", \"cat_1\", \"cat_2\", \"brand\"] >> Categorify() >> TagAsItemFeatures()\n",
    "\n",
    "# Target encode cat columns\n",
    "cat_groups =  nvt.ColumnSelector(['user_id', 'brand', 'cat_1', 'cat_2'])\n",
    "label = nvt.ColumnSelector([\"target\"])\n",
    "te_features = cat_groups >> TargetEncoding(label)\n",
    "te_features_norm = te_features >> Normalize() >> TagAsItemFeatures()\n",
    "\n",
    "price = (\n",
    "    ['price']\n",
    "    >> FillMissing(0)\n",
    "    >> LogOp()\n",
    "    >> Normalize()\n",
    "    >> TagAsItemFeatures()\n",
    ")   \n",
    "\n",
    "avg_category_id_pr = ['product_id'] >> JoinGroupby(cont_cols =['price'], stats=[\"mean\"]) >> Rename(name='avg_category_id_price')\n",
    "\n",
    "relative_price_to_avg_category = (\n",
    "    avg_category_id_pr \n",
    "    >> LambdaOp(relative_price_to_avg_categ, dependency=['price']) \n",
    "    >> Rename(name='relative_price')\n",
    "    >> AddMetadata(tags=[\"item\", Tags.CONTINUOUS])\n",
    ")\n",
    "\n",
    "\n",
    "target = (\n",
    "    [\"target\"] \n",
    "    >> nvt.ops.AddMetadata(tags=[Tags.BINARY_CLASSIFICATION, \"target\"]) \n",
    ")\n",
    "\n",
    "outputs = user_id  + context_features + item_id + item_features + price + relative_price_to_avg_category + count_logop_feat+ te_features_norm + target\n",
    "\n",
    "workflow = nvt.Workflow(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1e9f458-1ba2-46e1-b8d5-34143f72ab87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/cudf/core/frame.py:384: UserWarning: The deep parameter is ignored and is only included for pandas compatibility.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.16 s, sys: 3.86 s, total: 8.02 s\n",
      "Wall time: 8.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "workflow.fit(train_dataset)\n",
    "\n",
    "workflow.transform(train_dataset).to_parquet(\n",
    "    output_path=os.path.join(output_path, \"train\")\n",
    ")\n",
    "\n",
    "workflow.transform(valid_dataset).to_parquet(\n",
    "    output_path=os.path.join(output_path, \"valid\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08b08815-9878-46d1-b86b-99e96aaf9e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.read_parquet('./processed_nvt/train/part_0.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f678ee9-e014-4c05-9f49-403bbb45551d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>ts_hour</th>\n",
       "      <th>ts_weekday</th>\n",
       "      <th>ts_day</th>\n",
       "      <th>product_id</th>\n",
       "      <th>cat_0</th>\n",
       "      <th>cat_1</th>\n",
       "      <th>cat_2</th>\n",
       "      <th>brand</th>\n",
       "      <th>price</th>\n",
       "      <th>relative_price</th>\n",
       "      <th>product_id_count</th>\n",
       "      <th>TE_user_id_target</th>\n",
       "      <th>TE_brand_target</th>\n",
       "      <th>TE_cat_1_target</th>\n",
       "      <th>TE_cat_2_target</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1587740</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3723</td>\n",
       "      <td>0.022421</td>\n",
       "      <td>12.505351</td>\n",
       "      <td>-0.228408</td>\n",
       "      <td>0.516618</td>\n",
       "      <td>0.616164</td>\n",
       "      <td>0.992369</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1587740</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3723</td>\n",
       "      <td>0.022421</td>\n",
       "      <td>12.505351</td>\n",
       "      <td>-0.228408</td>\n",
       "      <td>0.518844</td>\n",
       "      <td>0.616738</td>\n",
       "      <td>0.992705</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  ts_hour  ts_weekday  ts_day  product_id  cat_0  cat_1  cat_2  \\\n",
       "0  1587740       24           2      18           1      1      1      1   \n",
       "1  1587740       24           2      18           1      1      1      1   \n",
       "\n",
       "   brand   price  relative_price  product_id_count  TE_user_id_target  \\\n",
       "0      1  0.3723        0.022421         12.505351          -0.228408   \n",
       "1      1  0.3723        0.022421         12.505351          -0.228408   \n",
       "\n",
       "   TE_brand_target  TE_cat_1_target  TE_cat_2_target  target  \n",
       "0         0.516618         0.616164         0.992369       0  \n",
       "1         0.518844         0.616738         0.992705       0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53260d1f-9ece-48eb-8f15-7617cbb3141e",
   "metadata": {},
   "source": [
    "NVTabular exported the schema file of our processed dataset. The schema.pbtxt is a protobuf text file contains features metadata, including statistics about features such as cardinality, min and max values and also tags based on their characteristics and dtypes (e.g., categorical, continuous, list, item_id). The metadata information is loaded from schema and their tags are used to automatically set the parameters of Merlin Models. In other words, Merlin Models relies on the schema object to automatically build all necessary input and output layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1a8f783-91e9-4890-801b-46adfe504d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/cudf/core/frame.py:384: UserWarning: The deep parameter is ignored and is only included for pandas compatibility.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train = Dataset(os.path.join(output_path, \"train\", \"*.parquet\"), part_size=\"500MB\")\n",
    "valid = Dataset(os.path.join(output_path, \"valid\", \"*.parquet\"), part_size=\"500MB\")\n",
    "\n",
    "# define schema object\n",
    "schema = train.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9130a100-8629-4688-929b-e6104a0bf63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68f54857-5c5e-4720-aa4b-dff9f81ef3a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'target'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_column = schema.select_by_tag(Tags.TARGET).column_names[0]\n",
    "target_column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4d14c7-8013-4f97-8721-bf51ffc2e73b",
   "metadata": {},
   "source": [
    "### DLRM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1889a80-4cc4-4b81-a81d-9311c70495b2",
   "metadata": {},
   "source": [
    "Deep Learning Recommendation Model (DLRM) architecture is a popular neural network model originally proposed by Facebook in 2019 as a personalization deep learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a952ede-bb09-4100-8bf2-025105f9bbd5",
   "metadata": {},
   "source": [
    "![DLRM](./images/DLRM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80af7f3-f22a-41eb-a58b-a907b6438f71",
   "metadata": {},
   "source": [
    "DLRM accepts two types of features: categorical and numerical.\n",
    "\n",
    "- For each categorical feature, an embedding table is used to provide dense representation to each unique value.\n",
    "- For numerical features, they are fed to model as dense features, and then transformed by a simple neural network referred to as \"bottom MLP\". This part of the network consists of a series of linear layers with ReLU activations.\n",
    "- The output of the bottom MLP and the embedding vectors are then fed into the dot product interaction operation (see Pairwise interaction step). The output of \"dot interaction\" is then concatenated with the features resulting from the bottom MLP (we apply a skip-connection there) and fed into the \"top MLP\" which is also a series of dense layers with activations ((a fully connected NN).\n",
    "- The model outputs a single number (here we use sigmoid function to generate probabilities) which can be interpreted as a likelihood of a certain user clicking on an ad, watching a movie, or viewing a news page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "416954ba-1f43-40ed-b3a5-d8003d0df756",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mm.DLRMModel(\n",
    "    schema,\n",
    "    embedding_dim=64,\n",
    "    bottom_block=mm.MLPBlock([128, 64]),\n",
    "    top_block=mm.MLPBlock([128, 64, 32]),\n",
    "    prediction_tasks=mm.BinaryClassificationTask(target_column),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d379252-5cb9-45ed-a2fb-6c804a2a7d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "2443/2443 [==============================] - 146s 48ms/step - loss: 0.4777 - auc: 0.8309 - regularization_loss: 0.0000e+00 - val_loss: 0.6524 - val_auc: 0.7087 - val_regularization_loss: 0.0000e+00\n",
      "Epoch 2/2\n",
      "2443/2443 [==============================] - 116s 47ms/step - loss: 0.3697 - auc: 0.9058 - regularization_loss: 0.0000e+00 - val_loss: 0.7171 - val_auc: 0.6961 - val_regularization_loss: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdd60063520>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', run_eagerly=False, metrics=[tf.keras.metrics.AUC()])\n",
    "model.fit(train, validation_data=valid, batch_size=4096, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806d989e-260a-4117-8536-28b3baee7ccd",
   "metadata": {},
   "source": [
    "### DCN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728c1919-39e1-4e2d-adc4-e612f6aa8046",
   "metadata": {},
   "source": [
    "DCN-V2 is an architecture proposed as an improvement upon the original [DCN model](https://arxiv.org/pdf/1708.05123.pdf). The explicit feature interactions of the inputs are learned through cross layers, and then combined with a deep network to learn complementary implicit interactions. The overall model architecture is depicted in Figure below, with two ways to combine the cross network with the deep network: (1) stacked and (2) parallel. The output of the embbedding layer is the concatenation of all the embedded vectors and the normalized dense features: x<sub>0</sub> = [x<sub>embed,1</sub>; . . . ; x<sub>embed,ùëõ</sub>; ùë•<sub>dense</sub>].\n",
    "\n",
    "![DCN](./images/DCN.png)\n",
    "\n",
    "<a href=\"https://arxiv.org/abs/2008.13535\">Image Source: DCN V2 paper</a>\n",
    "\n",
    "In this example, we build `DCN-v2 stacked` architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb1035e7-6306-4cc2-acb0-a06e005c2eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mm.DCNModel(\n",
    "    schema,\n",
    "    depth=2,\n",
    "    deep_block=mm.MLPBlock([64, 32]),\n",
    "    prediction_tasks=mm.BinaryClassificationTask(target_column),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b543fd5-e7cd-46c4-b786-ac9b22db981a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "   6/2443 [..............................] - ETA: 3:44 - loss: 0.6840 - auc_1: 0.5347 - regularization_loss: 0.0000e+00WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0418s vs `on_train_batch_end` time: 0.0445s). Check your callbacks.\n",
      "2443/2443 [==============================] - 125s 46ms/step - loss: 0.4443 - auc_1: 0.8580 - regularization_loss: 0.0000e+00 - val_loss: 0.6605 - val_auc_1: 0.6883 - val_regularization_loss: 0.0000e+00\n",
      "Epoch 2/2\n",
      "2443/2443 [==============================] - 111s 45ms/step - loss: 0.3048 - auc_1: 0.9375 - regularization_loss: 0.0000e+00 - val_loss: 0.7333 - val_auc_1: 0.6498 - val_regularization_loss: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd90b694760>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile('adam', run_eagerly=False, metrics=[tf.keras.metrics.AUC()])\n",
    "model.fit(train, validation_data=valid, batch_size=4096, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ece0d9b-e800-4326-9ba4-ff2507b2d959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1023/1023 [==============================] - 10s 9ms/step - loss: 0.7333 - auc_1: 0.6498 - regularization_loss: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.7332543134689331,\n",
       " 'auc_1': 0.6497794985771179,\n",
       " 'regularization_loss': 0.0}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(valid, batch_size=1024, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10a747ff-40da-4bea-9684-7c551690d0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "2443/2443 [==============================] - 124s 47ms/step - loss: 0.6293 - auc_2: 0.8586 - regularization_loss: 0.0000e+00 - val_loss: 0.6809 - val_auc_2: 0.6775 - val_regularization_loss: 0.0000e+00\n",
      "Epoch 2/2\n",
      "2443/2443 [==============================] - 112s 46ms/step - loss: 0.4177 - auc_2: 0.9403 - regularization_loss: 0.0000e+00 - val_loss: 0.8116 - val_auc_2: 0.6333 - val_regularization_loss: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd909eb5d90>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = mm.DCNModel(\n",
    "    schema,\n",
    "    depth=2,\n",
    "    deep_block=mm.MLPBlock([64, 32]),\n",
    "    prediction_tasks=mm.BinaryClassificationTask(target_column),\n",
    ")\n",
    "\n",
    "model.compile('adam', run_eagerly=False, metrics=[tf.keras.metrics.AUC()])\n",
    "model.fit(train, validation_data=valid, batch_size=4096, epochs=2, class_weight = {0: 1, 1: 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3af85b9-5b83-42b6-9cde-84870a2dfad7",
   "metadata": {},
   "source": [
    "**Improve DCN Model performance**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd862df-270f-4baf-98de-471668180bda",
   "metadata": {},
   "source": [
    "Hyperparameter tuning (optimization) is an important phenomenon to find the possible best sets of hyperparameters to build and train the model from a given dataset. Hyperparameter tuning can be done manually or be managed by an algorithm like grid search and bayesian optimization. The latter optimizes the search for the best hyperparameters guided by a metric that needs to be maximized or minimized.\n",
    "\n",
    "Below, we  showcase how we can inject certain hyperparameters to our DCN model and set their values to improve the model performance metrics, which is AUC for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d75a824-784d-40ea-adfa-905947991e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_decay_rate = 0.93\n",
    "optimizer = \"adam\"\n",
    "\n",
    "def get_optimizer():\n",
    "    lerning_rate =  0.008\n",
    "    if lr_decay_rate:\n",
    "        lerning_rate = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "            0.008,\n",
    "            decay_steps=100,\n",
    "            decay_rate=0.93,\n",
    "            staircase=True,\n",
    "        )\n",
    "\n",
    "    if optimizer == \"adam\":\n",
    "        opt = tf.keras.optimizers.Adam(\n",
    "            learning_rate=lerning_rate,\n",
    "        )\n",
    "\n",
    "    return opt\n",
    "\n",
    "opt = get_optimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efbb5608-3874-40da-9f00-f7867c28c377",
   "metadata": {},
   "outputs": [],
   "source": [
    "from merlin.models.utils import schema_utils\n",
    "embedding_dims = {}\n",
    "\n",
    "item_id_feature_name = schema.select_by_tag(Tags.ITEM_ID).column_names[0]\n",
    "item_id_domain = schema_utils.categorical_domains(schema)[\n",
    "    item_id_feature_name\n",
    "]\n",
    "embedding_dims[item_id_domain] = 128\n",
    "\n",
    "user_id_feature_name = schema.select_by_tag(Tags.USER_ID).column_names[0]\n",
    "user_id_domain = schema_utils.categorical_domains(schema)[\n",
    "    user_id_feature_name\n",
    "]\n",
    "embedding_dims[user_id_domain] = 128\n",
    "\n",
    "embedding_options = mm.EmbeddingOptions(\n",
    "    embedding_dims=embedding_dims,\n",
    "    infer_embedding_sizes=True,\n",
    "    embeddings_l2_reg=1e-5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37cecd18-c451-4e20-b183-4d0b99bb31e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mm.DCNModel(\n",
    "    schema,\n",
    "    depth=2,\n",
    "    deep_block=mm.MLPBlock(\n",
    "        [64, 32],\n",
    "        activation='relu',\n",
    "        no_activation_last_layer=False,\n",
    "        dropout=0.015,\n",
    "    ),\n",
    "    stacked=True,\n",
    "    embedding_options=embedding_options,\n",
    "    prediction_tasks=mm.BinaryClassificationTask('target'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3607350-f53c-4e4b-b402-2d631daf08c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "2443/2443 [==============================] - 193s 67ms/step - loss: 0.6178 - auc: 0.8712 - regularization_loss: 0.0133 - val_loss: 0.7135 - val_auc: 0.6844 - val_regularization_loss: 0.0078\n",
      "Epoch 2/2\n",
      "2443/2443 [==============================] - 164s 67ms/step - loss: 0.3207 - auc: 0.9686 - regularization_loss: 0.0172 - val_loss: 0.9698 - val_auc: 0.6374 - val_regularization_loss: 0.0078\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f57a0706f70>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=opt, run_eagerly=False, metrics=[tf.keras.metrics.AUC()])\n",
    "model.fit(train, validation_data=valid, batch_size=4096, epochs=2, class_weight = {0: 1, 1: 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450fe1ab-2e56-4378-b65f-e2ff1a48b3ea",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0bad8c-d55f-498c-9267-566ed00627e8",
   "metadata": {},
   "source": [
    "[XGBoost](https://xgboost.ai/), which stands for Extreme Gradient Boosting, is a scalable, distributed gradient-boosted decision tree (GBDT) machine learning library. It provides parallel tree boosting and is the leading machine learning library for regression, classification, and ranking problems.\n",
    "\n",
    "A Gradient Boosting Decision Trees (GBDT) is a decision tree ensemble learning algorithm similar to random forest, for classification and regression. Ensemble learning algorithms combine multiple machine learning algorithms to obtain a better model. Both random forest and GBDT build a model consisting of multiple decision trees. The difference is in how the trees are built and combined.\n",
    "\n",
    "The term ‚Äúgradient boosting‚Äù comes from the idea of ‚Äúboosting‚Äù or improving a single weak model by combining it with a number of other weak models in order to generate a collectively strong model. Gradient boosting is an extension of boosting where the process of additively generating weak models is formalized as a gradient descent algorithm over an objective function. Gradient boosting sets targeted outcomes for the next model in an effort to minimize errors. Targeted outcomes for each case are based on the gradient of the error (hence the name gradient boosting) with respect to the prediction.\n",
    "\n",
    "XGBoost is a scalable and highly accurate implementation of gradient boosting that pushes the limits of computing power for boosted tree algorithms, being built largely for energizing machine learning model performance and computational speed. With XGBoost, trees are built in parallel, instead of sequentially like GBDT. You can read more about XGBoost [here](https://www.nvidia.com/en-us/glossary/data-science/xgboost/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b71fed2-ce10-4f1d-940b-99c9ecce90f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from merlin.core.utils import Distributed\n",
    "from merlin.models.xgb import XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415865b9-52c7-44c4-9a41-97fc2a423d53",
   "metadata": {},
   "source": [
    "In order to facilitate training on data larger than the available GPU memory, the training will leverage Dask. All the complexity of starting a local dask cluster is hidden in the Distributed context manager.\n",
    "\n",
    "Without further ado, let's train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04fa0710-c0b9-49fc-b249-e1a5581ca1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_booster_params = {\n",
    "    'objective':'binary:logistic',\n",
    "    'tree_method':'gpu_hist',\n",
    "    'eval_metric': \"auc\"\n",
    "}\n",
    "\n",
    "xgb_train_params = {\n",
    "    'num_boost_round': 100,\n",
    "    'verbose_eval': 20,\n",
    "    'early_stopping_rounds': 100,\n",
    "}\n",
    "\n",
    "\n",
    "with Distributed():\n",
    "    model = XGBoost(schema=schema, **xgb_booster_params)\n",
    "    model.fit(\n",
    "        train,\n",
    "        evals=[(valid, 'validation_set'),],\n",
    "        **xgb_train_params\n",
    "    )\n",
    "    metrics = model.evaluate(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98790a32-4858-4956-bcf1-0c5ac319c45d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auc': 0.6393621079818755}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
