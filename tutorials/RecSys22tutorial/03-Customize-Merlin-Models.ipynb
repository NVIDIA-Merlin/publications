{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f62c3c8c-b14e-4933-8b70-fc5a833f2e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ====="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4cca29-4c9e-411d-8aa8-dd10b7d7aa00",
   "metadata": {},
   "source": [
    "## 3. Customize and Extend Merlin Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc9c8c6-9fab-435f-958b-923b0f3d6ff4",
   "metadata": {},
   "source": [
    "Merlin Models provides common and state-of-the-art RecSys architectures in a high-level API as well as all the required low-level building blocks (e.g., input blocks, MLP layers, prediction tasks, loss functions, etc.) for you to create your own architecture. \n",
    "\n",
    "In this lab, we define DLRM model architecture from scratch and customize it with Merlin Models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbe7611-8697-487d-983c-a8ec0187f773",
   "metadata": {},
   "source": [
    "**Learning Objectives**\n",
    "\n",
    "- Understand the building blocks of Merlin Models\n",
    "- Define DLRM model architecture with low-level api\n",
    "- Customize DLRM model with Merlin Models: \n",
    "    - Add cross-product transformation (see [Wide & Deep](https://arxiv.org/abs/1606.07792) paper) to the DLRM model.\n",
    "    - Replace the pairwise interaction layer of DLRM by a cross network (see [DCN-v2](https://arxiv.org/abs/2008.13535) paper).   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b54d3c6-fe6c-4a41-ab5e-c80c92efd91d",
   "metadata": {},
   "source": [
    "**Import Required Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1423d7a5-1ec7-4c9b-a65d-4ac6216f9817",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-09 15:56:58.965415: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-09 15:57:01.655431: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 16255 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB-LS, pci bus id: 0000:8a:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import glob\n",
    "import cudf \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nvtabular as nvt\n",
    "from nvtabular.ops import *\n",
    "import gc\n",
    "\n",
    "from merlin.schema.tags import Tags\n",
    "import merlin.models.tf as mm\n",
    "from merlin.io.dataset import Dataset\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a85c3587-4c2f-41b8-bf64-a15a6fe73c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e50199-066b-4475-8a9e-4ee0cc230b1b",
   "metadata": {},
   "source": [
    "Define data paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8efaea78-8cc2-4483-a200-21fbb88f1125",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/workspace/data/ecom/'\n",
    "output_path = os.path.join(data_path,'processed_nvt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372a3a0c-ba22-4efe-b32b-58b3833cfabd",
   "metadata": {},
   "source": [
    "Read processed parquet files as Dataset objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1a8f783-91e9-4890-801b-46adfe504d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/cudf/core/frame.py:384: UserWarning: The deep parameter is ignored and is only included for pandas compatibility.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:148: UserWarning: Compound tags like Tags.USER_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.USER: 'user'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:148: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train = Dataset(os.path.join(output_path, \"train\", \"*.parquet\"), part_size=\"500MB\")\n",
    "valid = Dataset(os.path.join(output_path, \"valid\", \"*.parquet\"), part_size=\"500MB\")\n",
    "\n",
    "# define schema object\n",
    "schema = train.schema.without(['event_time_ts', 'user_id_raw', 'product_id_raw'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68f54857-5c5e-4720-aa4b-dff9f81ef3a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'target'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_column = schema.select_by_tag(Tags.TARGET).column_names[0]\n",
    "target_column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cb8a3b-e60e-4815-9d72-c7d0e6c6ca6f",
   "metadata": {},
   "source": [
    "### 3.1. Introduction to Merlin Models' core building blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cbd690-fb76-4ce4-b52d-c9540a4cacf7",
   "metadata": {},
   "source": [
    "Let's explain the functions and blocks that we use to build our DLRM model using Merlin Models' blocks and methods:\n",
    "\n",
    "The `Block` is the core abstraction in Merlin Models and is the class from which all blocks inherit. The class extends the `tf.keras.layers.Layer` base class and implements a number of properties that simplify the creation of custom blocks and models. These properties include the Schema object for determining the embedding dimensions, input shapes, and output shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef933f6-6d90-4b28-92c3-e50f67a4f6dd",
   "metadata": {},
   "source": [
    "**Features Blocks** <br>\n",
    "\n",
    "`Embeddings:` Creates a ParallelBlock with an EmbeddingTable for each categorical feature in the schema. <br>\n",
    "`ContinuousFeatures:` Input block for continuous features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bbe8bf-43c5-4cc4-aa48-032eb51bdb3f",
   "metadata": {},
   "source": [
    "**Connects Methods** <br>\n",
    "The base class `Block` implements different connects methods that control how to link a given block to other blocks. In this lab, we use the following methods:\n",
    "\n",
    "- `connect:` Connect the block to other blocks sequentially. The output is a tensor returned by the last block.\n",
    "- `connect_with_shortcut:` Connect the block to other blocks sequentially and apply a skip connection with the block's output.\n",
    "\n",
    "To learn more about other `connect` methods you can visit [here](https://github.com/NVIDIA-Merlin/models/blob/main/merlin/models/tf/core/base.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5820ac-184c-4537-8d54-e29fb46f8855",
   "metadata": {},
   "source": [
    "### 3.2. Build a DLRM model using Merlin Models low-level API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba4ffcf-2bb4-40a3-9d6e-160fb7360bd3",
   "metadata": {},
   "source": [
    "In this lab, we follow the steps below build our DLRM model from scratch:\n",
    "\n",
    "- build bottom MLP block that processes numerical features\n",
    "- build embeddings layer for categorical features\n",
    "- combine the output of continuous and categorical blocks into one dictionary\n",
    "- apply a dot product between all continuous and categorical features to learn pairwise interactions\n",
    "- concat the resulting pairwise interaction with the deep representation of continuous features (skip-connection)\n",
    "- apply top MLP block with a series of dense layers to the concatenated tensor.\n",
    "\n",
    "<img src=\"./images/DLRM.png\" width=\"400\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38bcf17-ca8f-49d2-ada0-fd446f61ce50",
   "metadata": {},
   "source": [
    "We can feed an example batch through Merlin Models layers and get the results. So, let's  convert the first five rows of the valid dataset to a batch of input tensors, so that we can check out the outputs from each block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599f2326-d921-4085-886c-39b0e190fedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = mm.sample_batch(valid, batch_size=5, shuffle=False, include_targets=False)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efbdfc1-ea3b-4e48-8f4b-d4c0353f2c9a",
   "metadata": {},
   "source": [
    "We define the continuous layer based on the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e00940a-ea06-4cc5-8903-eec4dc632da8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'price': <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[1.0036721 ],\n",
       "        [0.86666507],\n",
       "        [1.2039791 ],\n",
       "        [0.64332473],\n",
       "        [1.0186374 ]], dtype=float32)>,\n",
       " 'relative_price': <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[-0.0670252 ],\n",
       "        [-0.03655583],\n",
       "        [ 0.08952092],\n",
       "        [-0.0848495 ],\n",
       "        [-0.0449205 ]], dtype=float32)>,\n",
       " 'TE_user_id_target': <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[0.657176  ],\n",
       "        [0.43412507],\n",
       "        [1.1355263 ],\n",
       "        [1.1355263 ],\n",
       "        [0.43412507]], dtype=float32)>,\n",
       " 'TE_brand_target': <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[0.36400566],\n",
       "        [0.42630896],\n",
       "        [0.27878827],\n",
       "        [0.52409333],\n",
       "        [0.36629993]], dtype=float32)>,\n",
       " 'TE_cat_1_target': <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[0.5736734 ],\n",
       "        [0.332066  ],\n",
       "        [0.57252383],\n",
       "        [0.57252383],\n",
       "        [0.57481176]], dtype=float32)>,\n",
       " 'TE_cat_2_target': <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 0.909615  ],\n",
       "        [-1.2208394 ],\n",
       "        [ 0.90870255],\n",
       "        [ 0.90870255],\n",
       "        [ 0.9101348 ]], dtype=float32)>}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "continuous_block = mm.ContinuousFeatures.from_schema(schema, tags=Tags.CONTINUOUS)\n",
    "continuous_block(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c78e389-f55d-4eba-82db-97ce12a93df2",
   "metadata": {},
   "source": [
    "We connect the continuous block to an `MLPBlock` to project them into the same dimensionality as the embedding width of categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47d2b697-7225-40c8-926a-057b4d350e1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 64), dtype=float32, numpy=\n",
       "array([[0.24168067, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.12457348, 0.06815903, 0.        , 0.33162716, 0.        ,\n",
       "        0.33803773, 0.3546738 , 0.18753652, 0.23609668, 0.        ,\n",
       "        0.        , 0.12242705, 0.        , 0.2776879 , 0.        ,\n",
       "        0.11242545, 0.41953292, 0.        , 0.        , 0.01660733,\n",
       "        0.11212933, 0.17494613, 0.2146942 , 0.43573064, 0.01884168,\n",
       "        0.03417264, 0.11854244, 0.6272503 , 0.        , 0.07257415,\n",
       "        0.        , 0.2819846 , 0.20729566, 0.        , 0.5770575 ,\n",
       "        0.        , 0.04364566, 0.00127549, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.1251218 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.35589513, 0.        , 0.        , 0.11736025, 0.        ,\n",
       "        0.16131279, 0.        , 0.23884481, 0.03296663],\n",
       "       [0.        , 0.24042037, 0.12846799, 0.19597757, 0.        ,\n",
       "        0.        , 0.06953666, 0.        , 0.14606173, 0.05001001,\n",
       "        0.        , 0.        , 0.        , 0.16134325, 0.32049423,\n",
       "        0.36069256, 0.        , 0.        , 0.44934723, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.43658325, 0.        ,\n",
       "        0.        , 0.37338737, 0.5206084 , 0.06577519, 0.        ,\n",
       "        0.4815748 , 0.        , 0.5796082 , 0.02441877, 0.15569223,\n",
       "        0.        , 0.10504699, 0.        , 0.19549197, 0.05214968,\n",
       "        0.2334048 , 0.        , 0.52903634, 0.        , 0.20281036,\n",
       "        0.32345647, 0.        , 0.        , 0.        , 0.19849572,\n",
       "        0.31364274, 0.04007414, 0.        , 0.38624278, 0.        ,\n",
       "        0.45386007, 0.        , 0.        , 0.4413787 , 0.5319444 ,\n",
       "        0.06094474, 0.        , 0.        , 0.24872525],\n",
       "       [0.12777638, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.17087898, 0.14164922, 0.        , 0.46039015, 0.        ,\n",
       "        0.4036094 , 0.36229193, 0.27167535, 0.36828482, 0.        ,\n",
       "        0.        , 0.31307364, 0.        , 0.37109223, 0.        ,\n",
       "        0.06714182, 0.59025866, 0.        , 0.01099394, 0.        ,\n",
       "        0.01036689, 0.2893343 , 0.34793794, 0.5059351 , 0.        ,\n",
       "        0.01073374, 0.06075973, 0.8073376 , 0.        , 0.15580666,\n",
       "        0.        , 0.24801612, 0.0848235 , 0.        , 0.71209437,\n",
       "        0.        , 0.00734527, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.12800768, 0.        , 0.        ,\n",
       "        0.        , 0.02909148, 0.        , 0.        , 0.        ,\n",
       "        0.43276024, 0.02051729, 0.        , 0.23271313, 0.04494125,\n",
       "        0.13397327, 0.        , 0.2764468 , 0.09085767],\n",
       "       [0.0891265 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.26828307, 0.10683779, 0.        , 0.31326592, 0.        ,\n",
       "        0.39727646, 0.38774952, 0.2156964 , 0.30587113, 0.        ,\n",
       "        0.        , 0.13085365, 0.        , 0.37026483, 0.        ,\n",
       "        0.10384186, 0.4850808 , 0.        , 0.0939216 , 0.        ,\n",
       "        0.        , 0.18921563, 0.24452955, 0.38977867, 0.        ,\n",
       "        0.        , 0.03808654, 0.682452  , 0.        , 0.09343616,\n",
       "        0.        , 0.12044507, 0.15614802, 0.        , 0.6188718 ,\n",
       "        0.        , 0.13975435, 0.        , 0.        , 0.        ,\n",
       "        0.08014757, 0.        , 0.07494862, 0.        , 0.        ,\n",
       "        0.        , 0.10361119, 0.13992397, 0.        , 0.        ,\n",
       "        0.38125885, 0.12353331, 0.        , 0.05997247, 0.        ,\n",
       "        0.        , 0.        , 0.3870921 , 0.14478321],\n",
       "       [0.29622543, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.0941954 , 0.05817264, 0.        , 0.2906809 , 0.        ,\n",
       "        0.29553473, 0.34688842, 0.16813764, 0.18690896, 0.02402675,\n",
       "        0.        , 0.07705817, 0.        , 0.22761077, 0.        ,\n",
       "        0.1347212 , 0.37450337, 0.        , 0.        , 0.07229865,\n",
       "        0.15425353, 0.13723405, 0.1530106 , 0.43814862, 0.03299801,\n",
       "        0.03486823, 0.15478614, 0.573315  , 0.        , 0.03916248,\n",
       "        0.        , 0.33004797, 0.24135074, 0.        , 0.5460639 ,\n",
       "        0.        , 0.0256541 , 0.02711064, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.11516422, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.31673798, 0.        , 0.        , 0.11626235, 0.01576817,\n",
       "        0.20246422, 0.        , 0.19008377, 0.        ]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bottom_block = continuous_block.connect(mm.MLPBlock([64]))\n",
    "bottom_block(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9c77c0-b8cd-4a07-a602-45b3dfc4f00d",
   "metadata": {},
   "source": [
    "We define the categorical embedding block based on the schema. We display the output tensor of the categorical embedding block using the data from the first batch. We can see the embeddings tensors of categorical features with a default dimension of 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7f6ef7-c516-458f-bf9c-11ceb76ea72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_block = mm.Embeddings(\n",
    "    schema.select_by_tag(Tags.CATEGORICAL),\n",
    "    dim = 64\n",
    ")\n",
    "embeddings_block(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83549e41-bc35-4f54-9d47-ac83d6b3d1c7",
   "metadata": {},
   "source": [
    "Let's combine the output of continuous and categorical blocks into one dictionary using a `ParallelBlock` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1eb9c543-1898-46bb-9239-46f78f80ef80",
   "metadata": {},
   "outputs": [],
   "source": [
    "dlrm_input_block = mm.ParallelBlock(\n",
    "    {\"embeddings\": embeddings_block, \"bottom_block\": bottom_block}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b539fb75-0124-4b57-b0cd-593f76de462c",
   "metadata": {},
   "source": [
    "By looking at the output, we can see that the ParallelBlock class applies embedding and continuous blocks, in parallel, to the same input batch. Additionally, it merges the resulting tensors into one dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddc10d12-4d5d-4aba-8961-96caf4ae74d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shapes of DLRM input block:\n",
      "\tuser_id : (5, 64)\n",
      "\tts_weekday : (5, 64)\n",
      "\tts_hour : (5, 64)\n",
      "\tproduct_id : (5, 64)\n",
      "\tcat_0 : (5, 64)\n",
      "\tcat_1 : (5, 64)\n",
      "\tcat_2 : (5, 64)\n",
      "\tbrand : (5, 64)\n",
      "\tbottom_block : (5, 64)\n"
     ]
    }
   ],
   "source": [
    "print(\"Output shapes of DLRM input block:\")\n",
    "for key, val in dlrm_input_block(batch).items():\n",
    "    print(\"\\t%s : %s\" % (key, val.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c07a42-ae9e-4f75-8915-6d11e8ef7c4f",
   "metadata": {},
   "source": [
    "**Define the interaction block**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cf6470-495e-475e-a8e0-d33970c9a2b3",
   "metadata": {},
   "source": [
    "Now that we have a vector representation of each input feature, we will create the DLRM interaction block. It consists of three operations:\n",
    "\n",
    "- Apply a dot product between all continuous and categorical features to learn pairwise interactions\n",
    "- Concat the resulting pairwise interaction with the deep representation of continuous features (skip-connection)\n",
    "- Apply an `MLPBlock` with a series of dense layers to the concatenated tensor.\n",
    "\n",
    "First, we use the `connect_with_shortcut` method to create first two operations of the DLRM interaction block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97493d5a-3f1e-4e2a-a985-a4400d1e0a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from merlin.models.tf.blocks.dlrm import DotProductInteractionBlock\n",
    "\n",
    "dlrm_interaction = dlrm_input_block.connect_with_shortcut(\n",
    "    DotProductInteractionBlock(), shortcut_filter=mm.Filter(\"bottom_block\"), aggregation=\"concat\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be103b68-4ec0-4216-a765-41ca901b312e",
   "metadata": {},
   "source": [
    "The `Filter` operation allows us to select the deep_continuous tensor from the dlrm_input_block outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2cfb12-334f-49b4-833b-e6f394ecd293",
   "metadata": {},
   "source": [
    "The following diagram provides a visualization of the operations that we constructed in the dlrm_interaction object.\n",
    "\n",
    "<img src=\"./images/residual_interaction.png\" width=\"300\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a91a9b3-0fe2-4f44-a088-124ef42bcec4",
   "metadata": {},
   "source": [
    "Print the tensor outputs from `dlrm_interaction` block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8794c83b-81ec-4f36-ad30-07e0a69c8c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "dlrm_interaction(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec9bff2a-5ca2-430d-9aed-4bbf408e65ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_mlp = mm.MLPBlock([128, 64, 32])\n",
    "dlrm_body = dlrm_interaction.connect(top_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600b6df6-81b8-4171-b339-efd258f2fced",
   "metadata": {},
   "source": [
    "**Define the Prediction block**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940dcd77-b9b4-44d0-bc82-72b573850673",
   "metadata": {},
   "source": [
    "At this stage, we have created the DLRM block that accepts a dictionary of categorical and continuous tensors as input. The output of this block is the interaction representation vector of shape 32. The next step is to use this hidden representation to conduct a given prediction task.\n",
    "\n",
    "We use the BinaryClassificationTask class and evaluate the performances using the AUC metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1ab00ac-2fc3-4165-8009-2d370ae217a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_task = mm.BinaryClassificationTask(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d315f8-7c5d-4959-9c3a-02ca3bed3491",
   "metadata": {},
   "source": [
    "We connect the deep DLRM interaction layer to the binary task head and the method automatically generates the Model class for us. We note that the Model class inherits from `tf.keras.Model` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9bbade0a-a3e3-485a-9de2-03c44d0a9097",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mm.Model(dlrm_body, binary_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c08ff2fc-492e-4247-90cf-c0e14858ce02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "735/735 [==============================] - 21s 17ms/step - loss: 0.5757 - auc: 0.7672 - regularization_loss: 0.0000e+00 - val_loss: 0.7220 - val_auc: 0.6388 - val_regularization_loss: 0.0000e+00\n",
      "Epoch 2/2\n",
      "735/735 [==============================] - 11s 14ms/step - loss: 0.4307 - auc: 0.8818 - regularization_loss: 0.0000e+00 - val_loss: 0.8016 - val_auc: 0.6311 - val_regularization_loss: 0.0000e+00\n",
      "CPU times: user 47 s, sys: 7.48 s, total: 54.5 s\n",
      "Wall time: 34.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fad775d8250>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "model.compile(optimizer='adam', run_eagerly=False, metrics=[tf.keras.metrics.AUC()])\n",
    "model.fit(train, validation_data=valid, batch_size=4096, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e8fa86-fc75-4859-b75f-3e99d7751cc8",
   "metadata": {},
   "source": [
    "### 3.3. Customize DLRM architecture: Add Cross-Product features to DLRM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3f1feb-1504-47e0-8c54-6883e50f6d15",
   "metadata": {},
   "source": [
    "We can synthetically form new features by multiplying (crossing) two or more sparse features. Crossing combinations of features can provide predictive abilities beyond what those features provide individually (see ref [website](https://developers.google.com/machine-learning/crash-course/feature-crosses/video-lecture#:~:text=A%20feature%20cross%20is%20a,an%20understanding%20of%20feature%20crosses)). In particular, cross-product feature transformations help the model memorize the niche/rare interactions seen in the user’s past history. Feature crossing has been used as an efficient technique in well-known DL architectures such as [Wide & Deep](https://arxiv.org/abs/1606.07792) in the Wide part. The `HashedCross` class in Merlin Models allows us to perform cross-product transformations between two or multiple sparse categorical features. By using the <i>hashing trick</i>, we can control the dimension of the resulting crossed features. Conceptually, the transformation can be thought of as: `hash(concatenation of features) % num_bins`. \n",
    "\n",
    "The purpose of this use case is to illustrate how two categorical variables can be combined to generate one cross-product feature. In more general situations, where you want to create multiple crossed features from a list of categorical variables, you can use the `HashCrossAll` class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0baa8a7-c951-4bbf-a871-0efc1f3ee5a7",
   "metadata": {},
   "source": [
    "Instead of re-building main blocks from scratch, at this step, we can use `DLRMBlock` instead to define the same DLRM architecture defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fb09ce4-cccf-47bf-a87e-ff9d9cf157e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dlrm_body = mm.DLRMBlock(schema, embedding_dim=64, bottom_block=mm.MLPBlock([128,64]), top_block= mm.MLPBlock([128, 64, 32]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8285b6f0-982e-483c-a741-2fa33f133716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 32), dtype=float32, numpy=\n",
       "array([[0.        , 0.02928996, 0.0473752 , 0.06288711, 0.        ,\n",
       "        0.0638621 , 0.        , 0.02099078, 0.        , 0.00656151,\n",
       "        0.01406717, 0.00405715, 0.02685894, 0.        , 0.08501423,\n",
       "        0.        , 0.        , 0.        , 0.04040261, 0.        ,\n",
       "        0.06904767, 0.00144228, 0.        , 0.        , 0.00687732,\n",
       "        0.        , 0.024181  , 0.03958534, 0.        , 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.02827137, 0.04741701, 0.        ,\n",
       "        0.01240498, 0.00037761, 0.06874807, 0.04542917, 0.        ,\n",
       "        0.0212361 , 0.        , 0.05115933, 0.        , 0.14823578,\n",
       "        0.        , 0.        , 0.        , 0.07170902, 0.01802336,\n",
       "        0.05887214, 0.00694745, 0.02424705, 0.        , 0.        ,\n",
       "        0.04553917, 0.03927574, 0.        , 0.08305362, 0.03873212,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.02731217, 0.05164035, 0.09959982, 0.        ,\n",
       "        0.07022698, 0.        , 0.04056957, 0.        , 0.03415136,\n",
       "        0.00853518, 0.00286614, 0.05249799, 0.        , 0.13875659,\n",
       "        0.        , 0.        , 0.        , 0.01803401, 0.        ,\n",
       "        0.10477301, 0.0188208 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.01477207, 0.05183046, 0.        , 0.01870842,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.06694078, 0.04770913, 0.06230027, 0.        ,\n",
       "        0.06163558, 0.        , 0.03133943, 0.        , 0.01458019,\n",
       "        0.01683656, 0.00883985, 0.01789494, 0.        , 0.08862974,\n",
       "        0.        , 0.        , 0.        , 0.04296421, 0.        ,\n",
       "        0.08128531, 0.01570249, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.02221729, 0.05271024, 0.        , 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.0172195 , 0.0535767 , 0.05303648, 0.        ,\n",
       "        0.06223053, 0.        , 0.01804297, 0.        , 0.0080241 ,\n",
       "        0.01247285, 0.        , 0.02850666, 0.        , 0.07800937,\n",
       "        0.        , 0.        , 0.        , 0.03384147, 0.        ,\n",
       "        0.06924057, 0.00031734, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.02317066, 0.03692444, 0.        , 0.00081753,\n",
       "        0.        , 0.01091799]], dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlrm_body(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5175d67-0634-4776-85fb-bcdf95489f4c",
   "metadata": {},
   "source": [
    "Define `cross_features` transformation block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a39548bc-d877-4255-855e-f462d98f632a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_schema = schema.select_by_name(names=[\"cat_0\", \"cat_1\"])\n",
    "cross_features = mm.HashedCross(cross_schema, num_bins=100, output_mode=\"one_hot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f045ddf8-e5ba-4136-ae1a-57d54a0e72dc",
   "metadata": {},
   "source": [
    "Print the tensor outputs from `cross_features` block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2aed13e8-9f3e-4881-8f3f-fd99b35a6006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cross_cat_0_cat_1': <tf.Tensor: shape=(5, 100), dtype=float32, numpy=\n",
       " array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.]], dtype=float32)>}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_features(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4916fdd1-2326-4d98-8cfc-a2fdd4eff59f",
   "metadata": {},
   "source": [
    "To learn more about the `HashedCross` class arguments (e.g., num_bins, output_mode, etc) you can visit [here](https://github.com/NVIDIA-Merlin/models/blob/main/merlin/models/tf/core/transformations.py#L825-L846)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef2fe76-7223-43f1-ba1c-4647d67f276a",
   "metadata": {},
   "source": [
    "For large cardinality, it is important to set `sparse=True` for `HashedCross`, `HashedCrossAll` and `CategoryEncoding`, as it makes output sparse (otherwise this might cause OOM). But for this example is fine to keep it as the default `sparse=False`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e62f7c8-4915-4e49-83d7-6e8c9e4b8aea",
   "metadata": {},
   "source": [
    "We add another feature interaction representation based on the weighted sum of feature crosses, thefore, we connect the `HashedCross` transformation block with a simple single-neuron linear MLP architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa0b1982-d047-4b8d-8ba0-ee1148d1fecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wide part: \n",
    "\n",
    "wide_body = cross_features.connect(\n",
    "    mm.MLPBlock([1], no_activation_last_layer=True), block_name='cross_model'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "164ae640-9ecf-4dc3-8fd0-4f4c8b5505bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       "array([[-0.15635493],\n",
       "       [ 0.2237919 ],\n",
       "       [-0.15635493],\n",
       "       [-0.15635493],\n",
       "       [-0.15635493]], dtype=float32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wide_body(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df54aba8-fbce-4f8f-972d-4fecdc2bd187",
   "metadata": {},
   "source": [
    "Concat `wide_body` layer to `dlrm_body` layer using `ParallelBlock`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "08fe4109-edc7-42cd-9f07-5e47a8249479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 33), dtype=float32, numpy=\n",
       "array([[ 0.        ,  0.02928996,  0.0473752 ,  0.06288711,  0.        ,\n",
       "         0.0638621 ,  0.        ,  0.02099078,  0.        ,  0.00656151,\n",
       "         0.01406717,  0.00405715,  0.02685894,  0.        ,  0.08501423,\n",
       "         0.        ,  0.        ,  0.        ,  0.04040261,  0.        ,\n",
       "         0.06904767,  0.00144228,  0.        ,  0.        ,  0.00687732,\n",
       "         0.        ,  0.024181  ,  0.03958534,  0.        ,  0.        ,\n",
       "         0.        ,  0.        , -0.15635493],\n",
       "       [ 0.        ,  0.        ,  0.02827137,  0.04741701,  0.        ,\n",
       "         0.01240498,  0.00037761,  0.06874807,  0.04542917,  0.        ,\n",
       "         0.0212361 ,  0.        ,  0.05115933,  0.        ,  0.14823578,\n",
       "         0.        ,  0.        ,  0.        ,  0.07170902,  0.01802336,\n",
       "         0.05887214,  0.00694745,  0.02424705,  0.        ,  0.        ,\n",
       "         0.04553917,  0.03927574,  0.        ,  0.08305362,  0.03873212,\n",
       "         0.        ,  0.        ,  0.2237919 ],\n",
       "       [ 0.        ,  0.02731217,  0.05164035,  0.09959982,  0.        ,\n",
       "         0.07022698,  0.        ,  0.04056957,  0.        ,  0.03415136,\n",
       "         0.00853518,  0.00286614,  0.05249799,  0.        ,  0.13875659,\n",
       "         0.        ,  0.        ,  0.        ,  0.01803401,  0.        ,\n",
       "         0.10477301,  0.0188208 ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.01477207,  0.05183046,  0.        ,  0.01870842,\n",
       "         0.        ,  0.        , -0.15635493],\n",
       "       [ 0.        ,  0.06694078,  0.04770913,  0.06230027,  0.        ,\n",
       "         0.06163558,  0.        ,  0.03133943,  0.        ,  0.01458019,\n",
       "         0.01683656,  0.00883985,  0.01789494,  0.        ,  0.08862974,\n",
       "         0.        ,  0.        ,  0.        ,  0.04296421,  0.        ,\n",
       "         0.08128531,  0.01570249,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.02221729,  0.05271024,  0.        ,  0.        ,\n",
       "         0.        ,  0.        , -0.15635493],\n",
       "       [ 0.        ,  0.0172195 ,  0.0535767 ,  0.05303648,  0.        ,\n",
       "         0.06223053,  0.        ,  0.01804297,  0.        ,  0.0080241 ,\n",
       "         0.01247285,  0.        ,  0.02850666,  0.        ,  0.07800937,\n",
       "         0.        ,  0.        ,  0.        ,  0.03384147,  0.        ,\n",
       "         0.06924057,  0.00031734,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.02317066,  0.03692444,  0.        ,  0.00081753,\n",
       "         0.        ,  0.01091799, -0.15635493]], dtype=float32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wide-and-dlrm\n",
    "wide_and_dlrm = mm.ParallelBlock({'wide':wide_body, \"dlrm\": dlrm_body}, aggregation=\"concat\")\n",
    "wide_and_dlrm(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "59c8b4ab-ef4a-41c1-b4f4-63a05abf15d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_task = mm.BinaryClassificationTask(schema)\n",
    "model = mm.Model(wide_and_dlrm, binary_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5a5388f-36ac-4210-978f-be1b870167c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "735/735 [==============================] - 20s 20ms/step - loss: 0.5723 - auc_1: 0.7713 - regularization_loss: 0.0000e+00 - val_loss: 0.7445 - val_auc_1: 0.6391 - val_regularization_loss: 0.0000e+00\n",
      "Epoch 2/2\n",
      "735/735 [==============================] - 14s 18ms/step - loss: 0.4417 - auc_1: 0.8748 - regularization_loss: 0.0000e+00 - val_loss: 0.8070 - val_auc_1: 0.6350 - val_regularization_loss: 0.0000e+00\n",
      "CPU times: user 53 s, sys: 7.1 s, total: 1min\n",
      "Wall time: 35.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f98c1dde1c0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "model.compile(optimizer='adam', run_eagerly=False, metrics=[tf.keras.metrics.AUC()])\n",
    "model.fit(train, validation_data=valid, batch_size=4096, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbb593a-7630-415b-a4ca-707d4821885d",
   "metadata": {},
   "source": [
    "### 3.4. Replace DotProductInteractionBlock with CrossBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5610910-2711-47cb-bb33-bc6e3754567e",
   "metadata": {},
   "source": [
    "In this section, we will replace DotProductInteractionBlock layer with `CrossBlock` that is used in [DCN-v2](https://arxiv.org/pdf/2008.13535.pdf) architecture. CrossBlock uses `Cross` layer which creates interactions of all input features. When used inside `CrossBlock`, stacked `Cross` layers can be used to do high-order features interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c0f315-6c5b-4ea2-9338-3feafdb386ea",
   "metadata": {},
   "source": [
    "We will keep bottom layer same as we did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5a909456-3b7d-4000-8cc5-73d75ce22b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_block = mm.ContinuousFeatures.from_schema(schema, tags=Tags.CONTINUOUS)\n",
    "bottom_block = continuous_block.connect(mm.MLPBlock([128,64]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5fbd8642-f428-4608-82c9-24b5ae5acfa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['user_id', 'ts_weekday', 'ts_hour', 'product_id', 'cat_0', 'cat_1', 'cat_2', 'brand']),\n",
       " TensorShape([5, 64]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_block = mm.Embeddings(\n",
    "    schema.select_by_tag(Tags.CATEGORICAL),\n",
    "    dim = 64\n",
    ")\n",
    "embeddings = embeddings_block(batch)\n",
    "embeddings.keys(), embeddings[\"user_id\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "25516092-a8b1-4b6b-9831-bc6ebb7ab759",
   "metadata": {},
   "outputs": [],
   "source": [
    "dlrm_input_block = mm.ParallelBlock(\n",
    "    {\"embeddings\": embeddings_block, \"bottom_block\": bottom_block},\n",
    "    aggregation=\"concat\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd25814-3c68-497a-bca4-7ea325c18dea",
   "metadata": {},
   "source": [
    "`CrossBlock` block provides a way to create high-order feature interactions by a number of stacked Cross Layers. We set the depth, which is the number of cross-layers to be stacked, to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1bfdf45e-d33e-428d-8cf7-bc14f08a0474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 576), dtype=float32, numpy=\n",
       "array([[ 0.01346518,  0.01996166,  0.10235769, ..., -0.01322006,\n",
       "        -0.00873787, -0.03701539],\n",
       "       [ 0.13175648,  0.03475271,  0.        , ..., -0.01474485,\n",
       "        -0.00794951, -0.03636587],\n",
       "       [ 0.        ,  0.05581063,  0.10123038, ..., -0.0129318 ,\n",
       "        -0.00880974, -0.03923511],\n",
       "       [ 0.        ,  0.04674373,  0.12530845, ..., -0.0126127 ,\n",
       "        -0.00837193, -0.04131868],\n",
       "       [ 0.01239133,  0.02843624,  0.11145297, ..., -0.01299973,\n",
       "        -0.00858843, -0.03969663]], dtype=float32)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stacked\n",
    "cross_inter_body = dlrm_input_block.connect(mm.CrossBlock(2))\n",
    "cross_inter_body(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4586b0-034f-4344-ae9c-3681ee3303a0",
   "metadata": {},
   "source": [
    "Concat `cross_inter_body` layer to bottom block using ParallelBlock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bded3273-e725-4695-9c59-6636c99f56dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dlrm_interaction = mm.ParallelBlock(\n",
    "    {\"cross_inter_body\": cross_inter_body, \"bottom_block\": bottom_block},\n",
    "    aggregation=\"concat\"\n",
    ")                                                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6577414f-c75a-45ba-a0ad-497671b51ddf",
   "metadata": {},
   "source": [
    "Then, we project the learned interaction using a series of dense layers, this defines the top block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f59d8df5-5c03-4f4c-84d4-ac61d22060f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_dlrm_interaction = dlrm_interaction.connect(mm.MLPBlock([128, 64, 32]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "267c9422-4f32-4b8c-bdaf-ffb8a36ca775",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_task = mm.BinaryClassificationTask(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dfa5cf-c504-40bf-91a6-610422a7c9a2",
   "metadata": {},
   "source": [
    "We connect the deep DLRM interaction layer to the binary task head, and automatically generate the Model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bff8aa41-cee8-40f3-b494-7bc31efce1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mm.Model(deep_dlrm_interaction, binary_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ff6cf67b-72d8-4402-80cc-c169efbe85e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "735/735 [==============================] - 18s 19ms/step - loss: 0.5677 - auc_2: 0.7759 - regularization_loss: 0.0000e+00 - val_loss: 0.7465 - val_auc_2: 0.6404 - val_regularization_loss: 0.0000e+00\n",
      "Epoch 2/2\n",
      "735/735 [==============================] - 12s 16ms/step - loss: 0.4573 - auc_2: 0.8644 - regularization_loss: 0.0000e+00 - val_loss: 0.8592 - val_auc_2: 0.6394 - val_regularization_loss: 0.0000e+00\n",
      "CPU times: user 46.4 s, sys: 6.91 s, total: 53.3 s\n",
      "Wall time: 32.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f98c1236a00>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "model.compile(optimizer='adam', run_eagerly=False, metrics=[tf.keras.metrics.AUC()])\n",
    "model.fit(train, validation_data=valid, batch_size=4096, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c502f0-1c0c-4e58-9801-a9b296dba374",
   "metadata": {},
   "source": [
    "### Summary "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e94162-fd41-477b-ab10-f2e603dc6bd2",
   "metadata": {},
   "source": [
    "In this hands-on lab we learned how to\n",
    "\n",
    "- use a subset of pre-existing blocks to create a DLRM model\n",
    "- add cross-product transformation block to the DLRM Model\n",
    "- replace DotProductInteractionBlock with CrossBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218e00ae-59e1-4934-ae69-dd05d13a246c",
   "metadata": {},
   "source": [
    "Please execute the cell below to shut down the kernel before moving on to the next notebook, `04-Building-multi-stage-RecSys-with-Merlin-Systems`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e4e28231-85dd-4f17-8243-75a4d9c94b1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
