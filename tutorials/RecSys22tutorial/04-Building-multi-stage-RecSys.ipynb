{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f62c3c8c-b14e-4933-8b70-fc5a833f2e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ====="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4cca29-4c9e-411d-8aa8-dd10b7d7aa00",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "## 3. Building Multi-Stage Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbbc0ed-0253-4f56-a368-a5536c77e8e9",
   "metadata": {},
   "source": [
    "Recommender Systems (RecSys) are the engine of the modern internet and the catalyst for human decisions. Building a recommendation system is challenging because it requires multiple stages (data preprocessing, offline training, item retrieval, filtering, ranking, ordering, etc.) to work together seamlessly and efficiently. The biggest challenges for new practitioners are the lack of understanding around what RecSys look like in the real world, and the gap between examples of simple models and a production-ready end-to-end recommender systems.\n",
    "\n",
    "The figure below represents a four-stage recommender systems. This is more complex process than only training a single model and deploying it, and it is much more realistic and closer to what's happening in the real-world recommender production systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e291586a-3464-4097-b4a3-9aecf05da8d9",
   "metadata": {},
   "source": [
    "<img src=\"./images/fourstage.png\" width=800 height=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785a0986-4e23-4969-9591-e1a92880c5ed",
   "metadata": {},
   "source": [
    "In this lab, we are going to showcase how we can deploy a multi-stage recommender systems using Merlin Systems library easily on Triton Inference Server. Let's go over the concepts in the figure briefly.\n",
    "\n",
    "- **Retrieval:** This is the step to narrow down millions of items into thousands of candidates. We are going to train a Two-Tower item retrieval model to retrieve the relevant top-K candidate items.\n",
    "- **Filtering:** This step is to exclude the already interacted or undesirable items from the candidate items set or to apply business logic rules. Although this is an important step, for this example we skip this step.\n",
    "- **Scoring:** This is also known as ranking. Here the retrieved and filtered candidate items are being scored. We are going to train a ranking model to be able to use at our scoring step.\n",
    "- **Ordering:** At this stage, we can order the final set of items that we want to recommend to the user. Here, weâ€™re able to align the output of the model with business needs, constraints, or criteria.\n",
    "\n",
    "To learn more about the four-stage recommender systems, you can listen to Even Oldridge's [Moving Beyond Recommender Models talk at KDD'21](https://www.youtube.com/watch?v=5qjiY-kLwFY&list=PL65MqKWg6XcrdN4TJV0K1PdLhF_Uq-b43&index=8) and read more in this [blog post](https://eugeneyan.com/writing/system-design-for-discovery/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbe7611-8697-487d-983c-a8ec0187f773",
   "metadata": {},
   "source": [
    "**Learning Objectives**\n",
    "\n",
    "- Train a ranking and retriveal model with [Merlin Models](https://github.com/NVIDIA-Merlin/models)\n",
    "- Export user query tower, user and item features, and item embedding\n",
    "- Create a feature store with Feast and register features to feature repo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba999b9-3b8c-4aba-a264-be6ef45ee4f9",
   "metadata": {},
   "source": [
    "**GOAL:** In this lab, we build and deploy a multi-stage recommender system to predict candidate items relevance scores, and then recommend top-k most relevant items for a given user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b54d3c6-fe6c-4a41-ab5e-c80c92efd91d",
   "metadata": {},
   "source": [
    "**Import Required Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1423d7a5-1ec7-4c9b-a65d-4ac6216f9817",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-13 15:34:17.270985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-13 15:34:17.272589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-13 15:34:17.273587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-13 15:34:17.297529: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-13 15:34:17.298217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-13 15:34:17.299229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-13 15:34:17.300191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-13 15:34:18.856068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-13 15:34:18.857133: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-13 15:34:18.858091: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-13 15:34:18.858987: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8080 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1e.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import glob\n",
    "import cudf \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nvtabular as nvt\n",
    "from nvtabular.ops import *\n",
    "import gc\n",
    "from datetime import datetime\n",
    "\n",
    "from merlin.schema.tags import Tags\n",
    "import merlin.models.tf as mm\n",
    "from merlin.io.dataset import Dataset\n",
    "from merlin.models.utils.dataset import unique_rows_by_features\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a85c3587-4c2f-41b8-bf64-a15a6fe73c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8efaea78-8cc2-4483-a200-21fbb88f1125",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/workspace/data/ecom/'\n",
    "output_path = os.path.join(data_path,'processed_nvt')\n",
    "output_path2 = os.path.join(data_path,'processed_filtered')\n",
    "BASE_DIR = os.environ.get(\n",
    "    \"BASE_DIR\", \"/workspace/recsys_tutorial/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372a3a0c-ba22-4efe-b32b-58b3833cfabd",
   "metadata": {},
   "source": [
    "Read processed parquet files as Dataset objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1a8f783-91e9-4890-801b-46adfe504d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/cudf/core/frame.py:384: UserWarning: The deep parameter is ignored and is only included for pandas compatibility.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:148: UserWarning: Compound tags like Tags.USER_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.USER: 'user'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:148: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_raw = Dataset(os.path.join(output_path, \"train\", \"*.parquet\"), part_size=\"500MB\")\n",
    "valid_raw = Dataset(os.path.join(output_path, \"valid\", \"*.parquet\"), part_size=\"500MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb6887b-837b-4fcf-a1f6-89cce471c6c4",
   "metadata": {},
   "source": [
    "**Filter out the negative rows**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ae7831-e49d-4d6e-a17d-134dab829cae",
   "metadata": {},
   "source": [
    "Here, we will filter our datasets with NVTabular `Filter()` operator to select only positive interaction rows where `target==1` in the dataset. We do that because we want to use `negative sampling` technique when training our candidate retrieval and ranking models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9aad817f-cf39-41a6-bd55-5a68b0e5194a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = train_raw.schema.column_names\n",
    "outputs = inputs >> Filter(f=lambda df: df[\"target\"] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e46a7e12-d840-4ddd-804a-f9d9b3c6aa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow2 = nvt.Workflow(outputs)\n",
    "\n",
    "workflow2.fit(train_raw)\n",
    "\n",
    "workflow2.transform(train_raw).to_parquet(\n",
    "    output_path=os.path.join(output_path2, \"train\")\n",
    ")\n",
    "\n",
    "workflow2.transform(valid_raw).to_parquet(\n",
    "    output_path=os.path.join(output_path2, \"valid\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4143efd7-deb6-4a31-bb03-4a6d6a1e2357",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow2.save(os.path.join(output_path2, \"workflow2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cb8a3b-e60e-4815-9d72-c7d0e6c6ca6f",
   "metadata": {},
   "source": [
    "### 3.1 Building and Training a Candidate Retrieval Model with Merlin Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c045cb-ded5-479a-80b4-6b0d0ebc5ea9",
   "metadata": {},
   "source": [
    "Industrial recommender systems have major tasks to accomplish that can be quite demanding. One requirement is to deliver a recommendation under the expected latency requirements (e.g., within milliseconds) to warrant a good user experience. That might require a significant amount of creativity and engineering. And the second consideration is that we might want to minimize infrastructure costs while solving the latency issue, which is yet another obstacle to overcome!\n",
    "\n",
    "In large scale recommender systems pipelines, the size of the item catalog (number of unique items) might be in the order of millions or billions. At such scale, a typical setup is having two-stage pipeline, where a faster candidate retrieval model quickly extracts thousands of relevant items and a then a more powerful ranking model (i.e. with more features and more powerful architecture) ranks the top-k items that are going to be displayed to the user. Therefore, industrial recommender systems usually consists of candidate retrieval and ranking (scoring) stages. The candidate retrieval stage retrieves candidate items that are relevant to user interests, while the ranking stage sorts candidate items by user interests.\n",
    "\n",
    "In this notebook, we start with the first stage of multi-stage recommender systems- the Candidate Retrieval. For ML-based candidate retrieval model, as it needs to quickly score millions of items for a given user, a popular choices are models that can produce recommendation scores by just computing the dot product the user embeddings and item embeddings. Popular choices of such models are `Matrix Factorization (MF)`, which learns low-rank user and item embeddings, and the `Two-Tower architecture`, which is a neural network with two MLP towers where both user and item features are fed to generate user and item embeddings in the output. Such models can be efficiently served by indexing the trained item embeddings into an Approximate Nearest Neighbors (ANN) engine and during inference scoring user embeddings over all indexed item embeddings within the engine. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bed922c-dd44-4a7a-8ee5-1275f7970150",
   "metadata": {},
   "source": [
    "#### Two-Tower Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2e80f4-81f9-47dd-9f01-602beec467f0",
   "metadata": {},
   "source": [
    "We are going to train a Two-Tower model for item retrieval. A Two-Tower Model consists of item (candidate) and user (query) encoder towers. With two towers, the model can learn representations (embeddings) for queries and candidates separately.\n",
    "\n",
    "\n",
    "<img src=\"./images/twotower.png\" width=400 height=400 />\n",
    "\n",
    "**Negative Sampling** <br>\n",
    "\n",
    "\n",
    "Many datasets for recommender systems contain implicit feedback with logs of user interactions like clicks, add-to-cart, purchases, music listening events, rather than explicit ratings that reflects user preferences over items. To be able to learn from implicit feedback, we use the general (and naive) assumption that the interacted items are more relevant for the user than the non-interacted ones. In Merlin Models we provide some scalable negative sampling algorithms for the Item Retrieval Task. In particular, we use in this example the in-batch sampling algorithm which uses the items interacted by other users as negatives within the same mini-batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee584e37-0f32-4ff8-af58-90ed13697619",
   "metadata": {},
   "source": [
    "Now, let's build our Two-Tower model. In a nutshell, we aggregate all user features to feed in user tower and feed the item features to the item tower. Then we compute the positive score by multiplying the user embedding with the item embedding and sample negative items (read more about negative sampling [here](https://openreview.net/pdf?id=824xC-SgWgU) and [here](https://medium.com/mlearning-ai/overview-negative-sampling-on-recommendation-systems-230a051c6cd7)), whose item embeddings are also multiplied by the user embedding. Then we apply the loss function on top of the positive and negative scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef062ef-9be9-4f24-bdfc-08a0b8d9ba64",
   "metadata": {},
   "source": [
    "**Read filtered parquet files as Dataset objects**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6be0a72-734e-4492-98ba-e86903505403",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tt = Dataset(os.path.join(output_path2, \"train\", \"*.parquet\"), part_size=\"500MB\")\n",
    "valid_tt = Dataset(os.path.join(output_path2, \"valid\", \"*.parquet\"), part_size=\"500MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842d6741-9104-4748-9f34-9061ffba18e6",
   "metadata": {},
   "source": [
    "Select user and item features using their corresponding tags, and exclude the columns that we do not want to train our retrieval model with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d032c486-3d9b-44f4-af5c-a290157b33cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = train_tt.schema.select_by_tag(\n",
    "    [Tags.ITEM_ID, Tags.USER_ID, Tags.ITEM, Tags.USER]).without(\n",
    "    ['event_time_ts', 'user_id_raw', 'product_id_raw', 'target']\n",
    ")\n",
    "train_tt.schema = schema\n",
    "valid_tt.schema = schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc38b1c7-a743-42f1-817e-99baa60a4920",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tt = mm.TwoTowerModel(\n",
    "    schema,\n",
    "    query_tower=mm.MLPBlock([128, 64], no_activation_last_layer=True),\n",
    "    samplers=[mm.InBatchSampler()],\n",
    "    embedding_options=mm.EmbeddingOptions(infer_embedding_sizes=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd29fce9-e3ea-49d3-b080-1fe10143ae14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x7f68aa4ce7c0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x7f68aa4ce7c0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The sampler InBatchSampler returned no samples for this batch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180/181 [============================>.] - ETA: 0s - loss: 8.5987 - recall_at_10: 0.0027 - ndcg_at_10: 0.0013 - regularization_loss: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The sampler InBatchSampler returned no samples for this batch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181/181 [==============================] - 23s 73ms/step - loss: 8.5986 - recall_at_10: 0.0028 - ndcg_at_10: 0.0013 - regularization_loss: 0.0000e+00 - val_loss: 8.3771 - val_recall_at_10: 0.0086 - val_ndcg_at_10: 0.0039 - val_regularization_loss: 0.0000e+00\n",
      "Epoch 2/3\n",
      "181/181 [==============================] - 12s 66ms/step - loss: 7.8333 - recall_at_10: 0.0184 - ndcg_at_10: 0.0086 - regularization_loss: 0.0000e+00 - val_loss: 8.2550 - val_recall_at_10: 0.0158 - val_ndcg_at_10: 0.0073 - val_regularization_loss: 0.0000e+00\n",
      "Epoch 3/3\n",
      "181/181 [==============================] - 13s 69ms/step - loss: 7.4442 - recall_at_10: 0.0389 - ndcg_at_10: 0.0186 - regularization_loss: 0.0000e+00 - val_loss: 8.3415 - val_recall_at_10: 0.0215 - val_ndcg_at_10: 0.0104 - val_regularization_loss: 0.0000e+00\n",
      "CPU times: user 2min 21s, sys: 3.21 s, total: 2min 24s\n",
      "Wall time: 48.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f66d49f7fd0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model_tt.compile(\n",
    "    optimizer=\"adam\",\n",
    "    run_eagerly=False,\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[mm.RecallAt(10), mm.NDCGAt(10)],\n",
    ")\n",
    "model_tt.fit(train_tt, validation_data=valid_tt, batch_size=1024 * 8, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958bac45-2d09-473a-b32e-26f5c04471eb",
   "metadata": {},
   "source": [
    "#### 3.1.1. Exporting query (user) model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb41840-acd3-4f04-9a50-f5133ce05d2c",
   "metadata": {},
   "source": [
    "We export the query tower to use it later during the model deployment stage with Merlin Systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5c75f14-1116-4ae6-8648-2faeb1f8f48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as model_context_layer_call_fn, model_context_layer_call_and_return_conditional_losses, parallel_block_1_layer_call_fn, parallel_block_1_layer_call_and_return_conditional_losses, sequential_block_3_layer_call_fn while saving (showing 5 of 38). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /dli/task/query_tower/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /dli/task/query_tower/assets\n"
     ]
    }
   ],
   "source": [
    "query_tower = model_tt.retrieval_block.query_block()\n",
    "query_tower.save(os.path.join(BASE_DIR, \"query_tower\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbb593a-7630-415b-a4ca-707d4821885d",
   "metadata": {},
   "source": [
    "### 3.2. Train a ranking Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0036aa-c7f0-439a-99bd-79bbe7aaa975",
   "metadata": {},
   "source": [
    "In this section we train DLRM architecture as our ranking (scoring) model with using negative sampling technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb121ea-6bca-4fbb-a328-c27b056ad997",
   "metadata": {},
   "source": [
    "<img src=\"./images/DLRM.png\" width=400 height=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c587a1d-b989-413b-a053-faa769e1f1be",
   "metadata": {},
   "source": [
    "**Read filtered parquet files as Dataset objects**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fb4f5f6-0171-4bbf-9aaa-bc3dc9bbb6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/cudf/core/frame.py:384: UserWarning: The deep parameter is ignored and is only included for pandas compatibility.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:148: UserWarning: Compound tags like Tags.USER_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.USER: 'user'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:148: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train = Dataset(os.path.join(output_path2, \"train\", \"*.parquet\"), part_size=\"500MB\")\n",
    "valid = Dataset(os.path.join(output_path2, \"valid\", \"*.parquet\"), part_size=\"500MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9804c3b-6508-4710-9375-c33d983220c6",
   "metadata": {},
   "source": [
    "Define schema object and remove columns from the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a1723ff-b1c5-4b43-a739-c4759d3b990b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'target'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema = train.schema.without(['event_time_ts', 'user_id_raw', 'product_id_raw'])\n",
    "train.schema = schema\n",
    "valid.schema = schema\n",
    "target_column = schema.select_by_tag(Tags.TARGET).column_names[0]\n",
    "target_column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31115ed-5a53-455b-a7ad-beaaafe518aa",
   "metadata": {},
   "source": [
    "In this section we are going to learn how we can train a ranking model with negative sampling method as we did for Two-Tower model. Even though we have the actual negatives in our example dataset, in this exercise, we showcase how one can use implicit feedback as a way of generating negatives. This time, we are going to use `UniformNegativeSampling` class for that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e4c5d1-ce6a-4f27-819d-85cb13e8483b",
   "metadata": {},
   "source": [
    "Augment the batch of positive interactions with `n_per_positive` negatives sampled from the same batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a909456-3b7d-4000-8cc5-73d75ce22b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from merlin.models.tf.data_augmentation.negative_sampling import UniformNegativeSampling\n",
    "from merlin.models.tf.dataset import BatchedDataset\n",
    "\n",
    "# do negative sampling on the fly\n",
    "batch_size, n_per_positive = 2048, 64\n",
    "add_negatives = UniformNegativeSampling(schema, n_per_positive, seed=42, return_tuple=True)\n",
    "dataset = BatchedDataset(train, batch_size=batch_size, shuffle=True)\n",
    "dataset = dataset.map(add_negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87afa6a-0309-4697-bd86-319672ed59e3",
   "metadata": {},
   "source": [
    "We can see that our train dataset only has positive interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5971c399-52a2-4453-b20d-38521ecae8dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1474912\n",
       "Name: target, dtype: int32"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.to_ddf().compute().target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3becd5-801b-4d7f-a8c7-18dbe1df040a",
   "metadata": {},
   "source": [
    "After negative sampling, we can check a batch and see that negatives are added to positive interactions. We have a batch with a shape of smaller than `batch_size + batch_size *n_per_positive` length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "676dd3db-d8a6-40fd-8a55-05b87fa2af09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]], shape=(132711, 1), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "inputs, target = next(iter(dataset))\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5aaa2313-e1cd-491a-92dc-bed30a8aa19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mm.DLRMModel(\n",
    "    schema,\n",
    "    embedding_dim=64,\n",
    "    bottom_block=mm.MLPBlock([128, 64]),\n",
    "    top_block=mm.MLPBlock([128, 64, 32]),\n",
    "    prediction_tasks=mm.BinaryClassificationTask(target_column),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc08e20e-4892-49cf-ac12-55f010e71644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "721/721 [==============================] - 67s 87ms/step - loss: 1.0862 - binary_accuracy: 0.6729 - auc: 0.7574 - regularization_loss: 0.0000e+00\n",
      "Epoch 2/2\n",
      "721/721 [==============================] - 63s 87ms/step - loss: 0.7044 - binary_accuracy: 0.8268 - auc: 0.9068 - regularization_loss: 0.0000e+00\n",
      "CPU times: user 2min 36s, sys: 16.2 s, total: 2min 52s\n",
      "Wall time: 2min 9s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1bc27d56a0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.compile(optimizer='adam', run_eagerly=False, metrics=[], \n",
    "              weighted_metrics=[tf.keras.metrics.BinaryAccuracy(),tf.keras.metrics.AUC()]\n",
    "             )\n",
    "model.fit(dataset, epochs=2, class_weight = {0: 1, 1: n_per_positive}, train_metrics_steps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eacde04-05e1-49c7-8e81-6067645c75af",
   "metadata": {},
   "source": [
    "We used `class_weight` arg to penalize more the misclassification made by the minority class (actual positive interactions) so the model doesn't get biased towards the majority class (sampled negatives). We evaluate the model using augmented, negative-sampling applied validation set assuming that we do not have explicit negatives in our dataset, which is the assumption in this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f38f2daa-d395-4a14-a9d4-99835ca6632a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 4s 69ms/step - loss: 0.8799 - binary_accuracy: 0.5714 - auc: 0.8281 - regularization_loss: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.8798810243606567,\n",
       " 'binary_accuracy': 0.5713640451431274,\n",
       " 'auc': 0.828089714050293,\n",
       " 'regularization_loss': 0.0}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dataset = BatchedDataset(valid, shuffle=False, batch_size = batch_size)\n",
    "valid_dataset = valid_dataset.map(add_negatives)\n",
    "metrics = model.evaluate(valid_dataset, return_dict=True)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5230917a-93f1-43e1-a24e-8ecff0a29800",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) TE_brand_target, TE_cat_1_target, TE_cat_2_target, TE_user_id_target with unsupported characters which will be renamed to te_brand_target, te_cat_1_target, te_cat_2_target, te_user_id_target in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as model_context_1_layer_call_fn, model_context_1_layer_call_and_return_conditional_losses, output_layer_layer_call_fn, output_layer_layer_call_and_return_conditional_losses, prediction_layer_call_fn while saving (showing 5 of 88). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /dli/task/dlrm/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /dli/task/dlrm/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(os.path.join(BASE_DIR, \"dlrm\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32593752-e229-44f0-9a01-b26b6a20fed3",
   "metadata": {},
   "source": [
    "### 3.3. Set up a feature store with Feast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94e871f2-67c4-477c-892c-e23f65d0450b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feast is an open source project that collects anonymized error reporting and usage statistics. To opt out or learn more see https://docs.feast.dev/reference/usage\n",
      "\n",
      "Creating a new Feast repository in \u001b[1m\u001b[32m/dli/task/feature_repo\u001b[0m.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!rm -rf $BASE_DIR/feature_repo\n",
    "!cd $BASE_DIR && feast init feature_repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6301fbd4-0f74-4013-b5a0-0bd5386d3f3a",
   "metadata": {},
   "source": [
    "[Feast](https://docs.feast.dev/) is an end-to-end open source feature store for machine learning. Feast (Feature Store) is a customizable operational data system that re-uses existing infrastructure to manage and serve machine learning features to real-time models.\n",
    "\n",
    "Below we create a new Feast repository called `feature_repo` under the `BASE_DIR`.\n",
    "\n",
    "You should be seeing a message like Creating a new Feast repository in ... printed out above. Now, we navigate to the feature_repo folder and remove the demo parquet file created by default, and `examples.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e5eaaa0e-aa8b-45af-bbd4-198776bf6b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_repo_path = os.path.join(BASE_DIR, \"feature_repo\")\n",
    "if os.path.exists(f\"{feature_repo_path}/example.py\"):\n",
    "    os.remove(f\"{feature_repo_path}/example.py\")\n",
    "if os.path.exists(f\"{feature_repo_path}/data/driver_stats.parquet\"):\n",
    "    os.remove(f\"{feature_repo_path}/data/driver_stats.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b935ad48-0598-4076-855c-285a69f15477",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features = (\n",
    "    unique_rows_by_features(train_raw, [Tags.USER,Tags.TIME], Tags.USER_ID)\n",
    "    .compute()\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b39d3f6-7a73-4812-9853-3cd373ea81e7",
   "metadata": {},
   "source": [
    "**unique_rows_by_features** : A utility function we can easily extract both unique user and item features tables as cuDF dataframes. The method extracts unique rows from a specified dataset (train_raw) based on a specified id-column tag (Tags.USER_ID), and the features to return are defined by `features_tag` ([Tags.USER,Tags.TIME])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d3419701-ac4e-4e86-873e-5af2d32b1a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features[\"datetime\"] = user_features[\"event_time_ts\"].astype(\"datetime64[ns]\")\n",
    "user_features[\"created\"] = datetime.now()\n",
    "user_features[\"created\"] = user_features[\"created\"].astype(\"datetime64[ns]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3107b61f-6415-493c-84cc-581f3ebb5f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features = user_features.drop(columns=['event_time_ts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d242721a-e26f-497f-9b76-6928440e9046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>ts_weekday</th>\n",
       "      <th>ts_hour</th>\n",
       "      <th>user_id_raw</th>\n",
       "      <th>datetime</th>\n",
       "      <th>created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>478741761</td>\n",
       "      <td>2020-03-15 11:47:05</td>\n",
       "      <td>2022-09-13 15:31:42.521536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>512402665</td>\n",
       "      <td>2020-01-13 11:47:29</td>\n",
       "      <td>2022-09-13 15:31:42.521536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>512416542</td>\n",
       "      <td>2020-03-29 13:19:05</td>\n",
       "      <td>2022-09-13 15:31:42.521536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>512454459</td>\n",
       "      <td>2020-03-23 12:35:18</td>\n",
       "      <td>2022-09-13 15:31:42.521536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>512487885</td>\n",
       "      <td>2020-03-29 11:14:14</td>\n",
       "      <td>2022-09-13 15:31:42.521536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  ts_weekday  ts_hour  user_id_raw            datetime  \\\n",
       "0        1           2        6    478741761 2020-03-15 11:47:05   \n",
       "1        2           4        6    512402665 2020-01-13 11:47:29   \n",
       "2        3           2        9    512416542 2020-03-29 13:19:05   \n",
       "3        4           4        8    512454459 2020-03-23 12:35:18   \n",
       "4        5           2        6    512487885 2020-03-29 11:14:14   \n",
       "\n",
       "                     created  \n",
       "0 2022-09-13 15:31:42.521536  \n",
       "1 2022-09-13 15:31:42.521536  \n",
       "2 2022-09-13 15:31:42.521536  \n",
       "3 2022-09-13 15:31:42.521536  \n",
       "4 2022-09-13 15:31:42.521536  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f48ba771-1103-4fe8-91fc-81940f8f95d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features.to_parquet(\n",
    "    os.path.join(BASE_DIR, \"feature_repo/data\", \"user_features.parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "24f2ae74-0cc0-47bd-9541-e049f389b002",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_features = (\n",
    "    unique_rows_by_features(train_raw, [Tags.ITEM, Tags.TIME], Tags.ITEM_ID)\n",
    "    .compute()\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "14bb74cf-94b4-4b5a-864f-580bb9bc46df",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_features[\"datetime\"] = item_features[\"event_time_ts\"].astype(\"datetime64[ns]\")\n",
    "item_features[\"created\"] = datetime.now()\n",
    "item_features[\"created\"] = item_features[\"created\"].astype(\"datetime64[ns]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "25fc4ad9-12d9-4833-9077-de0c4b401b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_features = item_features.drop(columns=['event_time_ts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "294c0a26-6257-43c1-9397-d46e44bfcfa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>cat_0</th>\n",
       "      <th>cat_1</th>\n",
       "      <th>cat_2</th>\n",
       "      <th>brand</th>\n",
       "      <th>price</th>\n",
       "      <th>relative_price</th>\n",
       "      <th>TE_user_id_target</th>\n",
       "      <th>TE_brand_target</th>\n",
       "      <th>TE_cat_1_target</th>\n",
       "      <th>TE_cat_2_target</th>\n",
       "      <th>product_id_raw</th>\n",
       "      <th>datetime</th>\n",
       "      <th>created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.472491</td>\n",
       "      <td>0.061859</td>\n",
       "      <td>0.434125</td>\n",
       "      <td>0.530555</td>\n",
       "      <td>0.574812</td>\n",
       "      <td>0.910135</td>\n",
       "      <td>1004767</td>\n",
       "      <td>2020-03-31 16:24:49</td>\n",
       "      <td>2022-09-13 15:31:42.826110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.546095</td>\n",
       "      <td>0.094918</td>\n",
       "      <td>-0.848214</td>\n",
       "      <td>0.431139</td>\n",
       "      <td>0.572524</td>\n",
       "      <td>0.908703</td>\n",
       "      <td>1005115</td>\n",
       "      <td>2020-03-31 14:33:00</td>\n",
       "      <td>2022-09-13 15:31:42.826110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id  cat_0  cat_1  cat_2  brand     price  relative_price  \\\n",
       "0           1      1      1      1      1  0.472491        0.061859   \n",
       "1           2      1      1      1      2  1.546095        0.094918   \n",
       "\n",
       "   TE_user_id_target  TE_brand_target  TE_cat_1_target  TE_cat_2_target  \\\n",
       "0           0.434125         0.530555         0.574812         0.910135   \n",
       "1          -0.848214         0.431139         0.572524         0.908703   \n",
       "\n",
       "   product_id_raw            datetime                    created  \n",
       "0         1004767 2020-03-31 16:24:49 2022-09-13 15:31:42.826110  \n",
       "1         1005115 2020-03-31 14:33:00 2022-09-13 15:31:42.826110  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_features.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c85ae0c2-220c-41db-9beb-cf93a204b7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to disk\n",
    "item_features.to_parquet(\n",
    "    os.path.join(BASE_DIR, \"feature_repo/data\", \"item_features.parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0d5c3d71-40f5-4bfe-abc3-0fb9ee206909",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) TE_brand_target, TE_cat_1_target, TE_cat_2_target, TE_user_id_target with unsupported characters which will be renamed to te_brand_target, te_cat_1_target, te_cat_2_target, te_user_id_target in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as model_context_layer_call_fn, model_context_layer_call_and_return_conditional_losses, parallel_block_layer_call_fn, parallel_block_layer_call_and_return_conditional_losses, sequential_block_4_layer_call_fn while saving (showing 5 of 42). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp4ix8rkiv/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp4ix8rkiv/assets\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:148: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "item_embs = model_tt.item_embeddings(\n",
    "    Dataset(item_features, schema=schema), batch_size=1024\n",
    ")\n",
    "item_embs_df = item_embs.compute(scheduler=\"synchronous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d66d1335-2129-4df8-ba6f-f4524790252e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only item_id together with embedding columns\n",
    "item_embeddings = item_embs_df.drop(\n",
    "    columns=['cat_0', 'cat_1', 'cat_2', 'brand', 'price',\n",
    "       'relative_price', 'TE_user_id_target', 'TE_brand_target',\n",
    "       'TE_cat_1_target', 'TE_cat_2_target']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b8287eba-6ab3-483f-8d40-726040227763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.297051</td>\n",
       "      <td>0.276046</td>\n",
       "      <td>-0.879044</td>\n",
       "      <td>0.027893</td>\n",
       "      <td>0.257287</td>\n",
       "      <td>0.652804</td>\n",
       "      <td>0.227450</td>\n",
       "      <td>-0.900420</td>\n",
       "      <td>-0.127272</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.841345</td>\n",
       "      <td>-1.075315</td>\n",
       "      <td>-0.321626</td>\n",
       "      <td>-0.566043</td>\n",
       "      <td>0.333218</td>\n",
       "      <td>0.089206</td>\n",
       "      <td>0.474416</td>\n",
       "      <td>-0.373821</td>\n",
       "      <td>-0.857889</td>\n",
       "      <td>-0.058564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.139945</td>\n",
       "      <td>-0.302786</td>\n",
       "      <td>-2.710636</td>\n",
       "      <td>-1.787605</td>\n",
       "      <td>0.935618</td>\n",
       "      <td>-0.452520</td>\n",
       "      <td>0.881097</td>\n",
       "      <td>-0.011493</td>\n",
       "      <td>-0.352558</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.806639</td>\n",
       "      <td>-1.484126</td>\n",
       "      <td>-1.056892</td>\n",
       "      <td>-2.276731</td>\n",
       "      <td>-0.114390</td>\n",
       "      <td>-2.039957</td>\n",
       "      <td>1.006160</td>\n",
       "      <td>-0.149321</td>\n",
       "      <td>-2.995681</td>\n",
       "      <td>-1.137149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id         0         1         2         3         4         5  \\\n",
       "0           1  0.297051  0.276046 -0.879044  0.027893  0.257287  0.652804   \n",
       "1           2 -1.139945 -0.302786 -2.710636 -1.787605  0.935618 -0.452520   \n",
       "\n",
       "          6         7         8  ...        54        55        56        57  \\\n",
       "0  0.227450 -0.900420 -0.127272  ... -0.841345 -1.075315 -0.321626 -0.566043   \n",
       "1  0.881097 -0.011493 -0.352558  ... -0.806639 -1.484126 -1.056892 -2.276731   \n",
       "\n",
       "         58        59        60        61        62        63  \n",
       "0  0.333218  0.089206  0.474416 -0.373821 -0.857889 -0.058564  \n",
       "1 -0.114390 -2.039957  1.006160 -0.149321 -2.995681 -1.137149  \n",
       "\n",
       "[2 rows x 65 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_embeddings.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "631ff4b6-3c32-4033-9263-c610cb39d52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to disk\n",
    "item_embeddings.to_parquet(os.path.join(BASE_DIR, \"item_embeddings.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a34912-3bd1-4182-b8e7-2ac5d46ba875",
   "metadata": {},
   "source": [
    "### 3.4. Create feature definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ff22ad-f280-48e9-985e-f71ddf297f44",
   "metadata": {},
   "source": [
    "Now we will create our user and item features definitions in the user_features.py and item_features.py files and save these files in the feature_repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9d82acf7-2c95-498f-8d7d-da7bc613a5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(os.path.join(BASE_DIR, \"feature_repo/\", \"user_features.py\"), \"w\")\n",
    "file.write(\n",
    "    \"\"\"\n",
    "from google.protobuf.duration_pb2 import Duration\n",
    "import datetime\n",
    "from feast import Entity, Feature, FeatureView, ValueType\n",
    "from feast.infra.offline_stores.file_source import FileSource\n",
    "\n",
    "user_features = FileSource(\n",
    "    path=\"{}\",\n",
    "    event_timestamp_column=\"datetime\",\n",
    "    created_timestamp_column=\"created\",\n",
    ")\n",
    "\n",
    "user_raw = Entity(name=\"user_id_raw\", value_type=ValueType.INT32, description=\"user id raw\",)\n",
    "\n",
    "user_features_view = FeatureView(\n",
    "    name=\"user_features\",\n",
    "    entities=[\"user_id_raw\"],\n",
    "    ttl=Duration(seconds=86400 * 7),\n",
    "    features=[\n",
    "        Feature(name=\"ts_weekday\", dtype=ValueType.INT32),\n",
    "        Feature(name=\"ts_hour\", dtype=ValueType.INT32),\n",
    "        Feature(name=\"user_id\", dtype=ValueType.INT32),\n",
    "    ],\n",
    "    online=True,\n",
    "    input=user_features,\n",
    "    tags=dict(),\n",
    ")\n",
    "\"\"\".format(\n",
    "        os.path.join(BASE_DIR, \"feature_repo/data/\", \"user_features.parquet\")\n",
    "    )\n",
    ")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8df12ab8-4d37-4fb2-911c-612353be31bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(BASE_DIR, \"feature_repo/\", \"item_features.py\"), \"w\") as f:\n",
    "    f.write(\n",
    "        \"\"\"\n",
    "from google.protobuf.duration_pb2 import Duration\n",
    "import datetime\n",
    "from feast import Entity, Feature, FeatureView, ValueType\n",
    "from feast.infra.offline_stores.file_source import FileSource\n",
    "\n",
    "item_features = FileSource(\n",
    "    path=\"{}\",\n",
    "    event_timestamp_column=\"datetime\",\n",
    "    created_timestamp_column=\"created\",\n",
    ")\n",
    "\n",
    "item = Entity(name=\"product_id\", value_type=ValueType.INT32, description=\"product id\",)\n",
    "\n",
    "item_features_view = FeatureView(\n",
    "    name=\"item_features\",\n",
    "    entities=[\"product_id\"],\n",
    "    ttl=Duration(seconds=86400 * 7),\n",
    "    features=[\n",
    "        Feature(name=\"cat_0\", dtype=ValueType.INT32),\n",
    "        Feature(name=\"cat_1\", dtype=ValueType.INT32),\n",
    "        Feature(name=\"cat_2\", dtype=ValueType.INT32),\n",
    "        Feature(name=\"brand\", dtype=ValueType.INT32),\n",
    "        Feature(name=\"price\", dtype=ValueType.FLOAT),\n",
    "        Feature(name=\"relative_price\", dtype=ValueType.FLOAT),\n",
    "        Feature(name=\"TE_user_id_target\", dtype=ValueType.FLOAT),\n",
    "        Feature(name=\"TE_brand_target\", dtype=ValueType.FLOAT),\n",
    "        Feature(name=\"TE_cat_1_target\", dtype=ValueType.FLOAT),\n",
    "        Feature(name=\"TE_cat_2_target\", dtype=ValueType.FLOAT),\n",
    "        Feature(name=\"product_id_raw\", dtype=ValueType.INT32),\n",
    "    ],\n",
    "    online=True,\n",
    "    input=item_features,\n",
    "    tags=dict(),\n",
    ")\n",
    "\"\"\".format(\n",
    "            os.path.join(BASE_DIR, \"feature_repo/data/\", \"item_features.parquet\")\n",
    "        )\n",
    "    )\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c502f0-1c0c-4e58-9801-a9b296dba374",
   "metadata": {},
   "source": [
    "### Summary "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e94162-fd41-477b-ab10-f2e603dc6bd2",
   "metadata": {},
   "source": [
    "In this hands-on lab we learned how\n",
    "- to train a Two-Tower model as candidate retrieval model using negative sampling technique \n",
    "- to train a DLRM model as ranking model using negative sampling technique \n",
    "- to export user, item features and item embeddings and save them\n",
    "- to set up a feature store using open-source tool FEAST and register features\n",
    "\n",
    "For first three steps, we used [Merlin Models](https://github.com/NVIDIA-Merlin/models) library.  Now we are ready to move on to our final lab where we will build an ensemble graph and deploy multiple models as an ensemble to Triton Inference Server [TIS](https://github.com/triton-inference-server/server) using [Merlin Systems](https://github.com/NVIDIA-Merlin/systems) library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26745168-3a1f-44e9-8aef-38e6cdf1f8d4",
   "metadata": {},
   "source": [
    "Please execute the cell below to shut down the kernel before moving on to the next notebook, `05-Deploying-multi-stage-RecSys-with-Merlin-Systems`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b431dd56-f3f9-4605-9b33-9082ef24e439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
